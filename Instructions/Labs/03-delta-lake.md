---
lab:
  title: Usar tabelas Delta no Apache Spark
  module: Work with Delta Lake tables in Microsoft Fabric
---

# Usar tabelas Delta no Apache Spark

As tabelas de um lakehouse do Microsoft Fabric s√£o baseadas no formato Delta Lake de c√≥digo aberto. O Delta Lake adiciona suporte para sem√¢ntica relacional para dados em lote e de streaming. Neste exerc√≠cio, voc√™ criar√° tabelas Delta e explorar√° os dados usando consultas SQL.

Este exerc√≠cio deve levar aproximadamente **45** minutos para ser conclu√≠do

> [!NOTE]
> Voc√™ precisa de uma avalia√ß√£o do [Microsoft Fabric](/fabric/get-started/fabric-trial) para concluir esse exerc√≠cio.

## Criar um workspace

Primeiro, crie um espa√ßo de trabalho com a *avalia√ß√£o do Fabric* habilitada.

1. Navegue at√© a [home page do Microsoft Fabric](https://app.fabric.microsoft.com/home?experience=fabric) em `https://app.fabric.microsoft.com/home?experience=fabric` em um navegador e entre com suas credenciais do Fabric.
1. Na barra de menus √† esquerda, selecione **Espa√ßos de Trabalho** (üóá).
1. Crie um **novo espa√ßo de trabalho** com um nome de sua escolha selecionando um modo de licenciamento que inclua a capacidade do Fabric (Avalia√ß√£o, Premium ou Fabric).
1. Quando o novo workspace for aberto, ele estar√° vazio.

    ![Imagem da tela de um espa√ßo de trabalho vazio do Fabric.](Images/workspace-empty.jpg)

## Criar um lakehouse e carregar dados

Agora que voc√™ tem um espa√ßo de trabalho, √© hora de criar um lakehouse e carregar alguns dados.

1. Na barra de menus √† esquerda, selecione **Criar**. Na p√°gina *Novo*, na se√ß√£o *Engenharia de Dados*, selecione **Lakehouse**. D√™ um nome exclusivo de sua prefer√™ncia.

    >**Observa√ß√£o**: se a op√ß√£o **Criar** n√£o estiver fixada na barra lateral, voc√™ precisar√° selecionar a op√ß√£o de retic√™ncias (**...**) primeiro.

1. H√° v√°rias maneiras de ingerir dados, mas neste exerc√≠cio, voc√™ baixar√° um arquivo de texto no computador local (ou na VM de laborat√≥rio, se aplic√°vel) e far√° seu upload no lakehouse. Baixe o [arquivo de dados](https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv) em `https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv`, salvando-o como *products.csv*.
1.  Retorne √† guia do navegador da Web que cont√©m seu lakehouse e, no painel do Explorer, ao lado da pasta **Arquivos**, clique no menu.  Crie uma **Nova subpasta** chamada *products*.
1.  No menu ... da pasta products, **carregue** o arquivo *products.csv* do computador local (ou da VM do laborat√≥rio, se aplic√°vel).
1.  Depois que o arquivo for carregado, selecione a pasta **products** e verifique se o arquivo foi carregado, conforme mostrado aqui:

    ![Imagem de tela de products.csv carregado no lakehouse.](Images/upload-products.jpg)
  
## Explorar dados em um DataFrame

1.  Crie um **Novo notebook**. Ap√≥s alguns segundos, um novo notebook que cont√©m uma s√≥ c√©lula ser√° aberto. Os notebooks s√£o compostos por uma ou mais c√©lulas que podem conter um c√≥digo ou um markdown (texto formatado).
2.  Selecione a primeira c√©lula (que atualmente √© uma c√©lula de c√≥digo) e, na barra de ferramentas no canto superior direito, use o bot√£o **M‚Üì** para convert√™-la em uma c√©lula Markdown. O texto contido na c√©lula ser√° ent√£o exibido como texto formatado. Use c√©lulas Markdown para fornecer informa√ß√µes explicativas sobre seu c√≥digo.
3.  Use o bot√£o üñâ (Editar) para alternar a c√©lula para o modo de edi√ß√£o e modifique o Markdown desta maneira:

    ```markdown
    # Delta Lake tables 
    Use this notebook to explore Delta Lake functionality 
    ```

4. Clique em qualquer lugar no notebook fora da c√©lula para parar de edit√°-lo e ver o markdown renderizado.
5. Adicione uma nova c√©lula de c√≥digo e o seguinte c√≥digo para ler os dados de produtos em um DataFrame usando um esquema definido:

    ```python
    from pyspark.sql.types import StructType, IntegerType, StringType, DoubleType

    # define the schema
    schema = StructType() \
    .add("ProductID", IntegerType(), True) \
    .add("ProductName", StringType(), True) \
    .add("Category", StringType(), True) \
    .add("ListPrice", DoubleType(), True)

    df = spark.read.format("csv").option("header","true").schema(schema).load("Files/products/products.csv")
    # df now is a Spark DataFrame containing CSV data from "Files/products/products.csv".
    display(df)
    ```

> [!TIP]
> Oculte ou exiba os pain√©is do explorador usando o √≠cone de divisa ¬´. Isso permite que voc√™ se concentre no notebook ou em seus arquivos.

7. Use o bot√£o **Executar c√©lula** (‚ñ∑) √† esquerda da c√©lula para execut√°-la.

> [!NOTE]
> Como esta √© a primeira vez que voc√™ executa qualquer c√≥digo Spark neste notebook, uma sess√£o do Spark precisa ser iniciada. Isso significa que a primeira execu√ß√£o pode levar alguns minutos para ser conclu√≠da. As execu√ß√µes seguintes ser√£o mais r√°pidas.

8. Quando o c√≥digo da c√©lula for conclu√≠do, analise a sa√≠da abaixo da c√©lula, que ser√° semelhante a esta:

    ![Imagem de tela dos dados de products.csv.](Images/products-schema.jpg)
 
## Criar tabelas Delta

Voc√™ pode salvar o DataFrame como uma tabela Delta usando o m√©todo *saveAsTable*. O Delta Lake d√° suporte √† cria√ß√£o de tabelas gerenciadas e externas.

   * As tabelas Delta **gerenciadas** se beneficiam de um desempenho mais alto, uma vez que o Fabric gerencia os metadados do esquema e os arquivos de dados.
   * As tabelas **externas** permitem que voc√™ armazene dados externamente, com os metadados gerenciados pelo Fabric.

### Criar uma tabela gerenciada

Os arquivos de dados s√£o criados na pasta **Tabelas**.

1. Nos resultados retornados pela primeira c√©lula de c√≥digo, use o √≠cone + C√≥digo para adicionar uma nova c√©lula de c√≥digo.

> [!TIP]
> Para ver o √≠cone + C√≥digo, posicione o mouse um pouco abaixo e √† esquerda da sa√≠da da c√©lula atual. Como alternativa, na barra de menus, na guia Editar, clique em **+ Adicionar c√©lula de c√≥digo**.

2. Para criar uma tabela Delta gerenciada, adicione uma c√©lula, insira o c√≥digo a seguir e execute a c√©lula:

    ```python
    df.write.format("delta").saveAsTable("managed_products")
    ```

3.  No painel do Lakehouse Explorer, **atualize** a pasta Tabelas e expanda o n√≥ Tabelas para verificar se a tabela **managed_products** foi criada.

>[!NOTE]
> O √≠cone de tri√¢ngulo ao lado do nome do arquivo indica uma tabela Delta.

Os arquivos da tabela gerenciada s√£o armazenados na pasta **Tabelas** no lakehouse. Uma pasta chamada *managed_products* foi criada e armazena os arquivos Parquet e a pasta delta_log da tabela.

### Criar uma tabela externa

Voc√™ tamb√©m pode criar tabelas externas, que podem ser armazenadas em outro lugar que n√£o o lakehouse, com os metadados do esquema armazenados no lakehouse.

1.  No painel do Lakehouse Explorer, no menu ... da pasta **Arquivos**, clique em **Copiar caminho do ABFS**. O caminho do ABFS √© o caminho totalmente qualificado para a pasta Arquivos do lakehouse.

2.  Em uma nova c√©lula de c√≥digo, cole o caminho do ABFS. Adicione o seguinte c√≥digo, usando recortar e colar para inserir o abfs_path no local correto no c√≥digo:

    ```python
    df.write.format("delta").saveAsTable("external_products", path="abfs_path/external_products")
    ```

3. O caminho completo ser√° parecido com este:

    ```python
    abfss://workspace@tenant-onelake.dfs.fabric.microsoft.com/lakehousename.Lakehouse/Files/external_products
    ```

4. **Execute** a c√©lula para salvar o DataFrame como uma tabela externa na pasta Files/external_products.

5.  No painel do Lakehouse Explorer, **atualize** a pasta Tabelas, expanda o n√≥ Tabelas e verifique se a tabela external_products foi criada contendo os metadados do esquema.

6.  No painel do Lakehouse Explorer, no menu ... da pasta Arquivos, clique em **Atualizar**. Em seguida, expanda o n√≥ Arquivos e verifique se a pasta external_products foi criada para os arquivos de dados da tabela.

### Comparar tabelas gerenciadas e externas

Vamos explorar as diferen√ßas entre as tabelas gerenciadas e externas usando o comando magic %%sql.

1. Em uma nova c√©lula de c√≥digo, adicione e execute o seguinte c√≥digo:

    ```python
    %%sql
    DESCRIBE FORMATTED managed_products;
    ```

2. Nos resultados, exiba a propriedade Local da tabela. Clique no valor Local na coluna Tipo de dados para ver o caminho completo. Observe que o local de armazenamento do OneLake termina com /Tables/managed_products.

3. Modifique o comando DESCRIBE para exibir os detalhes da tabela external_products conforme mostrado aqui:

    ```python
    %%sql
    DESCRIBE FORMATTED external_products;
    ```

4. Execute a c√©lula e, nos resultados, exiba a propriedade Local da tabela. Amplie a coluna Tipo de dados para ver o caminho completo e observe que os locais de armazenamento do OneLake terminam com /Files/external_products.

5. Em uma nova c√©lula de c√≥digo, adicione e execute o seguinte c√≥digo:

    ```python
    %%sql
    DROP TABLE managed_products;
    DROP TABLE external_products;
    ```

6. No painel do Lakehouse Explorer, **atualize** a pasta Tabelas para verificar se nenhuma tabela est√° listada no n√≥ Tabelas.
7.  No painel do Lakehouse Explorer, **atualize** a pasta Arquivos e verifique se o arquivo external_products *n√£o* foi exclu√≠do. Clique nessa pasta para exibir os arquivos de dados do Parquet e a pasta _delta_log. 

Os metadados da tabela externa foram exclu√≠dos, mas n√£o o arquivo de dados.

## Usar o SQL para criar uma tabela Delta

Agora voc√™ criar√° uma tabela Delta usando o comando magic %%sql. 

1. Adicione outra c√©lula de c√≥digo e execute o seguinte c√≥digo:

    ```python
    %%sql
    CREATE TABLE products
    USING DELTA
    LOCATION 'Files/external_products';
    ```

2. No painel do Lakehouse Explorer, no menu ... da pasta **Tabelas**, clique em **Atualizar**. Em seguida, expanda o n√≥ Tabelas e verifique se uma nova tabela chamada *products* est√° listada. Em seguida, expanda a tabela para exibir o esquema.

3. Adicione outra c√©lula de c√≥digo e execute o seguinte c√≥digo:

    ```python
    %%sql
    SELECT * FROM products;
    ```

## Explorar o controle de vers√£o de tabela

O hist√≥rico de transa√ß√µes das tabelas Delta √© armazenado nos arquivos JSON na pasta delta_log. Voc√™ pode usar esse log de transa√ß√µes para gerenciar o controle de vers√£o de dados.

1.  Adicione uma nova c√©lula de c√≥digo ao notebook e execute o c√≥digo a seguir, que implementa uma redu√ß√£o de 10% no pre√ßo das mountain bikes:

    ```python
    %%sql
    UPDATE products
    SET ListPrice = ListPrice * 0.9
    WHERE Category = 'Mountain Bikes';
    ```

2. Adicione outra c√©lula de c√≥digo e execute o seguinte c√≥digo:

    ```python
    %%sql
    DESCRIBE HISTORY products;
    ```

Os resultados mostram o hist√≥rico de transa√ß√µes registradas para a tabela.

3.  Adicione outra c√©lula de c√≥digo e execute o seguinte c√≥digo:

    ```python
    delta_table_path = 'Files/external_products'
    # Get the current data
    current_data = spark.read.format("delta").load(delta_table_path)
    display(current_data)

    # Get the version 0 data
    original_data = spark.read.format("delta").option("versionAsOf", 0).load(delta_table_path)
    display(original_data)
    ```

Dois conjuntos de resultado s√£o retornados ‚Äì um contendo os dados ap√≥s a redu√ß√£o de pre√ßo e o outro mostrando a vers√£o original dos dados.

## Analisar os dados da tabela Delta com consultas SQL

Usando o comando magic SQL, voc√™ pode usar a sintaxe SQL em vez do Pyspark. Aqui voc√™ criar√° uma exibi√ß√£o tempor√°ria da tabela products usando uma instru√ß√£o `SELECT`.

1. Adicione uma nova c√©lula de c√≥digo e execute o seguinte c√≥digo para criar e exibir a exibi√ß√£o tempor√°ria:

    ```python
    %%sql
    -- Create a temporary view
    CREATE OR REPLACE TEMPORARY VIEW products_view
    AS
        SELECT Category, COUNT(*) AS NumProducts, MIN(ListPrice) AS MinPrice, MAX(ListPrice) AS MaxPrice, AVG(ListPrice) AS AvgPrice
        FROM products
        GROUP BY Category;

    SELECT *
    FROM products_view
    ORDER BY Category;    
    ```

2. Adicione uma nova c√©lula de c√≥digo e execute o seguinte c√≥digo para retornar as dez principais categorias por n√∫mero de produtos:

    ```python
    %%sql
    SELECT Category, NumProducts
    FROM products_view
    ORDER BY NumProducts DESC
    LIMIT 10;
    ```

3. Quando os dados forem retornados, clique na exibi√ß√£o**Gr√°fico** para exibir um gr√°fico de barras.

    ![Imagem da tela da instru√ß√£o select SQL e dos resultados.](Images/sql-select.jpg)

Como alternativa, voc√™ pode executar uma consulta SQL usando o PySpark.

4. Adicione uma nova c√©lula de c√≥digo e execute o seguinte c√≥digo:

    ```python
    from pyspark.sql.functions import col, desc

    df_products = spark.sql("SELECT Category, MinPrice, MaxPrice, AvgPrice FROM products_view").orderBy(col("AvgPrice").desc())
    display(df_products.limit(6))
    ```

## Usar tabelas Delta para streaming de dados.

O Delta Lake permite streaming de dados. As tabelas delta podem ser um coletor ou uma fonte para fluxos de dados criados por meio da API de Streaming Estruturado do Spark. Neste exemplo, voc√™ usar√° uma tabela Delta como um coletor para streaming de dados em um cen√°rio simulado de IoT (Internet das Coisas).

1.  Adicione uma nova c√©lula de c√≥digo, o c√≥digo a seguir e execute:

    ```python
    from notebookutils import mssparkutils
    from pyspark.sql.types import *
    from pyspark.sql.functions import *

    # Create a folder
    inputPath = 'Files/data/'
    mssparkutils.fs.mkdirs(inputPath)

    # Create a stream that reads data from the folder, using a JSON schema
    jsonSchema = StructType([
    StructField("device", StringType(), False),
    StructField("status", StringType(), False)
    ])
    iotstream = spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger", 1).json(inputPath)

    # Write some event data to the folder
    device_data = '''{"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"error"}
    {"device":"Dev2","status":"ok"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}'''

    mssparkutils.fs.put(inputPath + "data.txt", device_data, True)

    print("Source stream created...")
    ```

Verifique que o texto *Fluxo de origem criado‚Ä¶* ser√° exibida. O c√≥digo que voc√™ acabou de executar criou uma fonte de dados de streaming com base em uma pasta na qual alguns dados foram salvos, representando leituras de dispositivos IoT hipot√©ticos.

2. Em uma nova c√©lula de c√≥digo, adicione e execute o seguinte c√≥digo:

    ```python
    # Write the stream to a delta table
    delta_stream_table_path = 'Tables/iotdevicedata'
    checkpointpath = 'Files/delta/checkpoint'
    deltastream = iotstream.writeStream.format("delta").option("checkpointLocation", checkpointpath).start(delta_stream_table_path)
    print("Streaming to delta sink...")
    ```

Esse c√≥digo grava os dados do dispositivo de streaming no formato Delta em uma pasta chamada iotdevicedata. Como o caminho para o local da pasta na pasta Tabelas, uma tabela ser√° criada automaticamente para ela.

3. Em uma nova c√©lula de c√≥digo, adicione e execute o seguinte c√≥digo:

    ```python
    %%sql
    SELECT * FROM IotDeviceData;
    ```

Esse c√≥digo consulta a tabela IotDeviceData, que cont√©m os dados do dispositivo da fonte de streaming.

4. Em uma nova c√©lula de c√≥digo, adicione e execute o seguinte c√≥digo:

    ```python
    # Add more data to the source stream
    more_data = '''{"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"error"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}'''

    mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)
    ```

Esse c√≥digo grava mais dados hipot√©ticos do dispositivo na fonte de streaming.

5. Execute novamente a c√©lula que cont√©m o seguinte c√≥digo:

    ```python
    %%sql
    SELECT * FROM IotDeviceData;
    ```

Esse c√≥digo consulta a tabela IotDeviceData novamente, que agora incluir√° os dados extras que foram adicionados √† fonte de streaming.

6. Em uma nova c√©lula de c√≥digo, adicione c√≥digo para interromper o fluxo e execute a c√©lula:

    ```python
    deltastream.stop()
    ```

## Limpar os recursos

Neste exerc√≠cio, voc√™ aprendeu a trabalhar com tabelas Delta no Microsoft Fabric.

Se voc√™ tiver terminado de explorar seu lakehouse, pode excluir o espa√ßo de trabalho criado para este exerc√≠cio.

1. Na barra √† esquerda, selecione o √≠cone do workspace para ver todos os itens que ele cont√©m.
2. No menu ... da barra de ferramentas, clique em **Configura√ß√µes do Espa√ßo de Trabalho**.
3. Na se√ß√£o Geral, clique em **Remover este espa√ßo de trabalho**.
