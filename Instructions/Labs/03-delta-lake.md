---
lab:
  title: Usar tabelas Delta no Apache Spark
  module: Work with Delta Lake tables in Microsoft Fabric
---

# Usar tabelas Delta no Apache Spark

As tabelas de um lakehouse do Microsoft Fabric s√£o baseadas no formato Delta Lake de c√≥digo aberto. O Delta Lake adiciona suporte para sem√¢ntica relacional para dados em lote e de streaming. Neste exerc√≠cio, voc√™ criar√° tabelas Delta e explorar√° os dados usando consultas SQL.

Este exerc√≠cio deve levar aproximadamente **45** minutos para ser conclu√≠do

> [!Note] 
> Para concluir este exerc√≠cio, voc√™ precisa de um [locat√°rio do Microsoft Fabric](https://learn.microsoft.com/fabric/get-started/fabric-trial).

## Criar um workspace

Antes de trabalhar com os dados no Fabric, crie um espa√ßo de trabalho em um locat√°rio com a capacidade do Fabric habilitada.

1. Navegue at√© a [home page do Microsoft Fabric](https://app.fabric.microsoft.com/home?experience=fabric-developer) em `https://app.fabric.microsoft.com/home?experience=fabric-developer` em um navegador e entre com suas credenciais do Fabric.
1. Na barra de menus √† esquerda, selecione **Workspaces** (o √≠cone √© semelhante a &#128455;).
1. Crie um workspace com um nome de sua escolha, selecionando um modo de licenciamento na se√ß√£o **Avan√ßado** que inclua a capacidade do Fabric (*Avalia√ß√£o*, *Premium* ou *Malha*).
1. Quando o novo workspace for aberto, ele estar√° vazio.

    ![Captura de tela de um espa√ßo de trabalho vazio no Fabric.](./Images/new-workspace.png)

## Criar um lakehouse e carregar arquivos

Agora que voc√™ tem um espa√ßo de trabalho, √© hora de criar um data lakehouse para seus dados.

1. Na barra de menus √† esquerda, selecione **Criar**. Na p√°gina *Novo*, na se√ß√£o *Engenharia de Dados*, selecione **Lakehouse**. D√™ um nome exclusivo de sua prefer√™ncia.

    >**Observa√ß√£o**: se a op√ß√£o **Criar** n√£o estiver fixada na barra lateral, voc√™ precisar√° selecionar a op√ß√£o de retic√™ncias (**...**) primeiro.

    Ap√≥s alguns minutos, um lakehouse ser√° criado:

    ![Captura de tela de um novo lakehouse.](./Images/new-lakehouse.png)

1. Visualize o novo lakehouse e observe que o painel do **Explorador** √† esquerda permite navegar pelas tabelas e arquivos no lakehouse:

Voc√™ j√° pode ingerir dados no lakehouse. H√° v√°rias maneiras de fazer isso, por enquanto, voc√™ baixar√° uma pasta de arquivo de texto no computador local (ou na VM de laborat√≥rio, se aplic√°vel) e far√° seu upload no lakehouse. 

1. Baixe o [arquivo de dados](https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv) em `https://github.com/MicrosoftLearning/dp-data/raw/main/products.csv`, salvando-o como *products.csv*.
1. Retorne √† guia do navegador da Web que cont√©m seu lakehouse e, no painel do Explorer, ao lado da pasta **Arquivos**, clique no menu.  Crie uma **Nova subpasta** chamada *products*.
1. No menu ... da pasta products, **carregue** o arquivo *products.csv* do computador local (ou da VM do laborat√≥rio, se aplic√°vel).
1. Depois que o arquivo for carregado, selecione a pasta **products** e verifique se o arquivo foi carregado, conforme mostrado aqui:

    ![Imagem de tela de products.csv carregado no lakehouse.](Images/upload-products.png)
  
## Explorar dados em um DataFrame

Agora voc√™ pode criar um notebook do Fabric para trabalhar com seus dados. Os notebooks fornecem um ambiente interativo no qual voc√™ pode escrever e executar c√≥digo.

1. Na barra de menus √† esquerda, selecione **Criar**. Na p√°gina *Novo*, na se√ß√£o *Engenharia de Dados*, selecione **Notebook**.

    Um novo notebook chamado **Notebook 1** √© criado e aberto.

    ![Captura de tela de um novo notebook.](./Images/new-notebook.png)

1. O Fabric atribui um nome a cada notebook criado, como Bloco de Notebook 1, Notebook 2, etc. Clique no painel de nome acima da guia **P√°gina Inicial** no menu para alterar o nome para algo mais descritivo.
1. Selecione a primeira c√©lula (que atualmente √© uma c√©lula de c√≥digo) e, na barra de ferramentas no canto superior direito, use o bot√£o **M‚Üì** para convert√™-la em uma c√©lula Markdown. O texto contido na c√©lula ser√° ent√£o exibido como texto formatado.
1. Use o bot√£o üñâ (Editar) para alternar a c√©lula para o modo de edi√ß√£o e modifique o Markdown como mostrado abaixo.

    ```markdown
    # Delta Lake tables 
    Use this notebook to explore Delta Lake functionality 
    ```

1. Clique em qualquer lugar do notebook fora da c√©lula para parar de edit√°-lo.
1. No painel do **Explorer**, selecione **Adicionar itens de dados** e selecione **Fontes de dados existentes**. Conecte-se ao lakehouse que voc√™ criou anteriormente.
1. Adicione uma nova c√©lula de c√≥digo e o seguinte c√≥digo para ler os dados de produtos em um DataFrame usando um esquema definido:

    ```python
   from pyspark.sql.types import StructType, IntegerType, StringType, DoubleType

   # define the schema
   schema = StructType() \
   .add("ProductID", IntegerType(), True) \
   .add("ProductName", StringType(), True) \
   .add("Category", StringType(), True) \
   .add("ListPrice", DoubleType(), True)

   df = spark.read.format("csv").option("header","true").schema(schema).load("Files/products/products.csv")
   # df now is a Spark DataFrame containing CSV data from "Files/products/products.csv".
   display(df)
    ```

> [!TIP]
> Oculte ou exiba os pain√©is do explorador usando o √≠cone de divisa ¬´. Isso permite que voc√™ se concentre no notebook ou em seus arquivos.

1. Use o bot√£o **Executar c√©lula** (‚ñ∑) √† esquerda da c√©lula para execut√°-la.

> [!NOTE]
> Como esta √© a primeira vez que voc√™ executa qualquer c√≥digo Spark neste notebook, uma sess√£o do Spark precisa ser iniciada. Isso significa que a primeira execu√ß√£o pode levar alguns minutos para ser conclu√≠da. As execu√ß√µes seguintes ser√£o mais r√°pidas.

1. Quando o c√≥digo da c√©lula for conclu√≠do, analise a sa√≠da abaixo da c√©lula, que ser√° semelhante a esta:

    ![Imagem de tela dos dados de products.csv.](Images/products-schema.png)
 
## Criar tabelas Delta

Voc√™ pode salvar o DataFrame como uma tabela Delta usando o m√©todo *saveAsTable*. O Delta Lake d√° suporte √† cria√ß√£o de tabelas gerenciadas e externas.

   * As tabelas Delta **gerenciadas** se beneficiam de um desempenho mais alto, uma vez que o Fabric gerencia os metadados do esquema e os arquivos de dados.
   * As tabelas **externas** permitem que voc√™ armazene dados externamente, com os metadados gerenciados pelo Fabric.

### Criar uma tabela gerenciada

Os arquivos de dados s√£o criados na pasta **Tabelas**.

1. Nos resultados retornados pela primeira c√©lula de c√≥digo, use o √≠cone + C√≥digo para adicionar uma nova c√©lula de c√≥digo.

> [!TIP]
> Para ver o √≠cone + C√≥digo, posicione o mouse um pouco abaixo e √† esquerda da sa√≠da da c√©lula atual. Como alternativa, na barra de menus, na guia Editar, clique em **+ Adicionar c√©lula de c√≥digo**.

1. Para criar uma tabela Delta gerenciada, adicione uma c√©lula, insira o c√≥digo a seguir e execute a c√©lula:

    ```python
   df.write.format("delta").saveAsTable("managed_products")
    ```

1. No painel do Explorador, **atualize** a pasta Tabelas e expanda o n√≥ Tabelas para verificar se a tabela **managed_products** foi criada.

> [!NOTE]
> O √≠cone de tri√¢ngulo ao lado do nome do arquivo indica uma tabela Delta.

Os arquivos da tabela gerenciada s√£o armazenados na pasta **Tabelas** no lakehouse. Uma pasta chamada *managed_products* foi criada e armazena os arquivos Parquet e a pasta delta_log da tabela.

### Criar uma tabela externa

Voc√™ tamb√©m pode criar tabelas externas, que podem ser armazenadas em outro lugar que n√£o o lakehouse, com os metadados do esquema armazenados no lakehouse.

1. No painel do Explorador, no menu ... da pasta **Arquivos**, clique em **Copiar caminho do ABFS**. O caminho do ABFS √© o caminho totalmente qualificado para a pasta Arquivos do lakehouse.

1. Em uma nova c√©lula de c√≥digo, cole o caminho do ABFS. Adicione o seguinte c√≥digo, usando recortar e colar para inserir o abfs_path no local correto no c√≥digo:

    ```python
   df.write.format("delta").saveAsTable("external_products", path="abfs_path/external_products")
    ```

1. O caminho completo ser√° parecido com este:

    ```python
   abfss://workspace@tenant-onelake.dfs.fabric.microsoft.com/lakehousename.Lakehouse/Files/external_products
    ```

1. **Execute** a c√©lula para salvar o DataFrame como uma tabela externa na pasta Files/external_products.

1. No painel do Explorador, **atualize** a pasta Tabelas, expanda o n√≥ Tabelas e verifique se a tabela external_products foi criada contendo os metadados do esquema.

1. No painel do Explorador, no menu ... da pasta Arquivos, clique em **Atualizar**. Em seguida, expanda o n√≥ Arquivos e verifique se a pasta external_products foi criada para os arquivos de dados da tabela.

### Comparar tabelas gerenciadas e externas

Vamos explorar as diferen√ßas entre as tabelas gerenciadas e externas usando o comando magic %%sql.

1. Em uma nova c√©lula de c√≥digo, adicione e execute o seguinte c√≥digo:

    ```python
   %%sql
   DESCRIBE FORMATTED managed_products;
    ```

1. Nos resultados, exiba a propriedade Local da tabela. Clique no valor Local na coluna Tipo de dados para ver o caminho completo. Observe que o local de armazenamento do OneLake termina com /Tables/managed_products.

1. Modifique o comando DESCRIBE para exibir os detalhes da tabela external_products conforme mostrado aqui:

    ```python
   %%sql
   DESCRIBE FORMATTED external_products;
    ```

1. Execute a c√©lula e, nos resultados, exiba a propriedade Local da tabela. Amplie a coluna Tipo de dados para ver o caminho completo e observe que os locais de armazenamento do OneLake terminam com /Files/external_products.

1. Em uma nova c√©lula de c√≥digo, adicione e execute o seguinte c√≥digo:

    ```python
   %%sql
   DROP TABLE managed_products;
   DROP TABLE external_products;
    ```

1. No painel do Explorador, **atualize** a pasta Tabelas para verificar se n√£o h√° tabelas listadas no n√≥ Tabelas.
1. No painel do Explorador, **atualize** a pasta Arquivos e verifique se o arquivo external_products *n√£o* foi exclu√≠do. Clique nessa pasta para exibir os arquivos de dados do Parquet e a pasta _delta_log. 

Os metadados da tabela externa foram exclu√≠dos, mas n√£o o arquivo de dados.

## Usar o SQL para criar uma tabela Delta

Agora voc√™ criar√° uma tabela Delta usando o comando magic %%sql. 

1. Adicione outra c√©lula de c√≥digo e execute o seguinte c√≥digo:

    ```python
   %%sql
   CREATE TABLE products
   USING DELTA
   LOCATION 'Files/external_products';
    ```

1. No painel do Explorador, no menu ... da pasta **Tabelas**, clique em **Atualizar**. Em seguida, expanda o n√≥ Tabelas e verifique se uma nova tabela chamada *products* est√° listada. Em seguida, expanda a tabela para exibir o esquema.
1. Adicione outra c√©lula de c√≥digo e execute o seguinte c√≥digo:

    ```python
   %%sql
   SELECT * FROM products;
    ```

## Explorar o controle de vers√£o de tabela

O hist√≥rico de transa√ß√µes das tabelas Delta √© armazenado nos arquivos JSON na pasta delta_log. Voc√™ pode usar esse log de transa√ß√µes para gerenciar o controle de vers√£o de dados.

1. Adicione uma nova c√©lula de c√≥digo ao notebook e execute o c√≥digo a seguir, que implementa uma redu√ß√£o de 10% no pre√ßo das mountain bikes:

    ```python
   %%sql
   UPDATE products
   SET ListPrice = ListPrice * 0.9
   WHERE Category = 'Mountain Bikes';
    ```

1. Adicione outra c√©lula de c√≥digo e execute o seguinte c√≥digo:

    ```python
   %%sql
   DESCRIBE HISTORY products;
    ```

Os resultados mostram o hist√≥rico de transa√ß√µes registradas para a tabela.

1. Adicione outra c√©lula de c√≥digo e execute o seguinte c√≥digo:

    ```python
   delta_table_path = 'Files/external_products'
   # Get the current data
   current_data = spark.read.format("delta").load(delta_table_path)
   display(current_data)

   # Get the version 0 data
   original_data = spark.read.format("delta").option("versionAsOf", 0).load(delta_table_path)
   display(original_data)
    ```

Dois conjuntos de resultado s√£o retornados ‚Äì um contendo os dados ap√≥s a redu√ß√£o de pre√ßo e o outro mostrando a vers√£o original dos dados.

## Analisar os dados da tabela Delta com consultas SQL

Usando o comando magic SQL, voc√™ pode usar a sintaxe SQL em vez do Pyspark. Aqui voc√™ criar√° uma exibi√ß√£o tempor√°ria da tabela products usando uma instru√ß√£o `SELECT`.

1. Adicione uma nova c√©lula de c√≥digo e execute o seguinte c√≥digo para criar e exibir a exibi√ß√£o tempor√°ria:

    ```python
   %%sql
   -- Create a temporary view
   CREATE OR REPLACE TEMPORARY VIEW products_view
   AS
       SELECT Category, COUNT(*) AS NumProducts, MIN(ListPrice) AS MinPrice, MAX(ListPrice) AS MaxPrice, AVG(ListPrice) AS AvgPrice
       FROM products
       GROUP BY Category;

   SELECT *
   FROM products_view
   ORDER BY Category;    
    ```

1. Adicione uma nova c√©lula de c√≥digo e execute o seguinte c√≥digo para retornar as dez principais categorias por n√∫mero de produtos:

    ```python
   %%sql
   SELECT Category, NumProducts
   FROM products_view
   ORDER BY NumProducts DESC
   LIMIT 10;
    ```

1. Quando os dados forem retornados, selecione **+ Novo gr√°fico** para exibir um dos gr√°ficos sugeridos.

    ![Imagem da tela da instru√ß√£o select SQL e dos resultados.](Images/sql-select.png)

Como alternativa, voc√™ pode executar uma consulta SQL usando o PySpark.

1. Adicione uma nova c√©lula de c√≥digo e execute o seguinte c√≥digo:

    ```python
   from pyspark.sql.functions import col, desc

   df_products = spark.sql("SELECT Category, MinPrice, MaxPrice, AvgPrice FROM products_view").orderBy(col("AvgPrice").desc())
   display(df_products.limit(6))
    ```

## Usar tabelas Delta para streaming de dados.

O Delta Lake permite streaming de dados. As tabelas delta podem ser um coletor ou uma fonte para fluxos de dados criados por meio da API de Streaming Estruturado do Spark. Neste exemplo, voc√™ usar√° uma tabela Delta como um coletor para streaming de dados em um cen√°rio simulado de IoT (Internet das Coisas).

1.  Adicione uma nova c√©lula de c√≥digo, o c√≥digo a seguir e execute:

    ```python
    from notebookutils import mssparkutils
    from pyspark.sql.types import *
    from pyspark.sql.functions import *

    # Create a folder
    inputPath = 'Files/data/'
    mssparkutils.fs.mkdirs(inputPath)

    # Create a stream that reads data from the folder, using a JSON schema
    jsonSchema = StructType([
    StructField("device", StringType(), False),
    StructField("status", StringType(), False)
    ])
    iotstream = spark.readStream.schema(jsonSchema).option("maxFilesPerTrigger", 1).json(inputPath)

    # Write some event data to the folder
    device_data = '''{"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}
    {"device":"Dev1","status":"error"}
    {"device":"Dev2","status":"ok"}
    {"device":"Dev2","status":"error"}
    {"device":"Dev1","status":"ok"}'''

    mssparkutils.fs.put(inputPath + "data.txt", device_data, True)

    print("Source stream created...")
    ```

Verifique que o texto *Fluxo de origem criado‚Ä¶* ser√° exibida. O c√≥digo que voc√™ acabou de executar criou uma fonte de dados de streaming com base em uma pasta na qual alguns dados foram salvos, representando leituras de dispositivos IoT hipot√©ticos.

1. Em uma nova c√©lula de c√≥digo, adicione e execute o seguinte c√≥digo:

    ```python
   # Write the stream to a delta table
   delta_stream_table_path = 'Tables/iotdevicedata'
   checkpointpath = 'Files/delta/checkpoint'
   deltastream = iotstream.writeStream.format("delta").option("checkpointLocation", checkpointpath).start(delta_stream_table_path)
   print("Streaming to delta sink...")
    ```

Esse c√≥digo grava os dados do dispositivo de streaming no formato Delta em uma pasta chamada iotdevicedata. Como o caminho para o local da pasta na pasta Tabelas, uma tabela ser√° criada automaticamente para ela.

1. Em uma nova c√©lula de c√≥digo, adicione e execute o seguinte c√≥digo:

    ```python
   %%sql
   SELECT * FROM IotDeviceData;
    ```

Esse c√≥digo consulta a tabela IotDeviceData, que cont√©m os dados do dispositivo da fonte de streaming.

1. Em uma nova c√©lula de c√≥digo, adicione e execute o seguinte c√≥digo:

    ```python
   # Add more data to the source stream
   more_data = '''{"device":"Dev1","status":"ok"}
   {"device":"Dev1","status":"ok"}
   {"device":"Dev1","status":"ok"}
   {"device":"Dev1","status":"ok"}
   {"device":"Dev1","status":"error"}
   {"device":"Dev2","status":"error"}
   {"device":"Dev1","status":"ok"}'''

   mssparkutils.fs.put(inputPath + "more-data.txt", more_data, True)
    ```

Esse c√≥digo grava mais dados hipot√©ticos do dispositivo na fonte de streaming.

1. Execute novamente a c√©lula que cont√©m o seguinte c√≥digo:

    ```python
   %%sql
   SELECT * FROM IotDeviceData;
    ```

Esse c√≥digo consulta a tabela IotDeviceData novamente, que agora incluir√° os dados extras que foram adicionados √† fonte de streaming.

1. Em uma nova c√©lula de c√≥digo, adicione c√≥digo para interromper o fluxo e execute a c√©lula:

    ```python
   deltastream.stop()
    ```

## Limpar os recursos

Neste exerc√≠cio, voc√™ aprendeu a trabalhar com tabelas Delta no Microsoft Fabric.

Se voc√™ tiver terminado de explorar seu lakehouse, pode excluir o espa√ßo de trabalho criado para este exerc√≠cio.

1. Na barra √† esquerda, selecione o √≠cone do workspace para ver todos os itens que ele cont√©m.
1. No menu ... da barra de ferramentas, clique em **Configura√ß√µes do Espa√ßo de Trabalho**.
1. Na se√ß√£o Geral, clique em **Remover este espa√ßo de trabalho**.
