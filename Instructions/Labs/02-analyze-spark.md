---
lab:
  title: Analisar dados com Apache Spark
  module: Use Apache Spark to work with files in a lakehouse
---

# Analisar dados com Apache Spark no Fabric

Neste laborat√≥rio, voc√™ ingerir√° dados no lakehouse do Fabric e usar√° o PySpark para ler e analisar os dados.

Este laborat√≥rio levar√° aproximadamente 45 minutos para ser conclu√≠do.

## Pr√©-requisitos

* Uma [avalia√ß√£o do Microsoft Fabric](https://www.microsoft.com/microsoft-fabric/getting-started).

## Criar um workspace

Antes de trabalhar com dados no Fabric, voc√™ precisa criar um espa√ßo de trabalho.

1. Na home page do [Microsoft Fabric](https://app.fabric.microsoft.com) em https://app.fabric.microsoft.com, escolha a experi√™ncia **Engenharia de Dados**.
1. Na barra de navega√ß√£o √† esquerda, selecione **Espa√ßos de trabalho** (üóá) e **Novo espa√ßo de trabalho**.
1. D√™ um nome ao novo espa√ßo de trabalho e, na se√ß√£o **Avan√ßado**, escolha o Modo de licenciamento apropriado. Se voc√™ tiver iniciado uma avalia√ß√£o do Microsoft Fabric, escolha Avalia√ß√£o.
1. Clique em **Aplicar** para criar um espa√ßo de trabalho vazio.
 
![Imagem da tela dos arquivos CSV carregados em um novo espa√ßo de trabalho do Fabric.](Images/uploaded-files.jpg)

## Criar um lakehouse e carregar arquivos

Agora que tem um espa√ßo de trabalho, voc√™ pode criar um lakehouse para seus arquivos de dados. No novo workspace, clique em **Novo** e **Lakehouse**. D√™ um nome ao lakehouse e clique em **Criar**. Ap√≥s um pequeno atraso, um novo lakehouse ser√° criado.

Voc√™ j√° pode ingerir dados no lakehouse. H√° v√°rias maneiras de fazer isso, mas por ora, voc√™ baixar√° uma pasta de arquivos de texto no computador local (ou na VM de laborat√≥rio, se aplic√°vel) e far√° seu upload no lakehouse.

1. Baixe os arquivos de dados em https://github.com/MicrosoftLearning/dp-data/raw/main/orders.zip.
1. Extraia o arquivo compactado e verifique se voc√™ tem uma pasta chamada *orders* com tr√™s arquivos CSV: 2019.csv, 2020.csv e 2021.csv.
1. Volte para o novo lakehouse. No painel do **Explorer**, clique no menu de retic√™ncias (**...**) ao lado da pasta **Arquivos** e clique em **Carregar** e **Carregar pasta**. Navegue at√© a pasta de pedidos em seu computador local (ou VM de laborat√≥rio, se aplic√°vel) e clique em **Carregar**.
1. Depois que os arquivos forem carregados, expanda **Arquivos** e clique na pasta **orders**. Verifique se os arquivo CSV foram carregados, conforme mostrado aqui:

![Imagem de tela de um novo espa√ßo de trabalho do Fabric.](Images/new-workspace.jpg)

## Criar um notebook

Agora voc√™ pode criar um notebook do Fabric para trabalhar com seus dados. Os notebooks fornecem um ambiente interativo no qual voc√™ pode escrever e executar c√≥digo.

1. Escolha o seu espa√ßo de trabalho e clique em **Novo** e **Notebook**. Ap√≥s alguns segundos, um novo notebook que cont√©m uma s√≥ c√©lula ser√° aberto. Os notebooks s√£o compostos por uma ou mais c√©lulas que podem conter um c√≥digo ou um markdown (texto formatado).
1. O Fabric atribui um nome a cada notebook criado, como Bloco de Notebook 1, Notebook 2, etc. Clique no painel de nome acima da guia **P√°gina Inicial** no menu para alterar o nome para algo mais descritivo.
1. Selecione a primeira c√©lula (que atualmente √© uma c√©lula de c√≥digo) e, na barra de ferramentas no canto superior direito, use o bot√£o **M‚Üì** para convert√™-la em uma c√©lula Markdown. O texto contido na c√©lula ser√° ent√£o exibido como texto formatado.
1. Use o bot√£o üñâ (Editar) para alternar a c√©lula para o modo de edi√ß√£o e modifique o Markdown como mostrado abaixo.

```markdown
# Sales order data exploration
Use this notebook to explore sales order data
```
![Imagem de tela de um notebook Fabric com uma c√©lula Markdown.](Images/name-notebook-markdown.jpg)

Quando terminar, clique em qualquer lugar no notebook fora da c√©lula para parar de edit√°-lo e ver o Markdown renderizado.


## Criar um DataFrame

Agora que voc√™ criou um espa√ßo de trabalho, um lakehouse e um notebook, voc√™ pode trabalhar com seus dados. Voc√™ usar√° o PySpark, que √© a linguagem padr√£o para notebooks do Fabric, e a vers√£o do Python otimizada para o Spark.

**OBSERVA√á√ÉO:** os notebooks do Fabric aceitam v√°rias linguagens de programa√ß√£o, incluindo Scala, R e Spark SQL.

1. Selecione seu novo espa√ßo de trabalho na barra √† esquerda. Voc√™ ver√° uma lista de itens contidos no espa√ßo de trabalho, incluindo seu lakehouse e notebook.
2. Selecione o lakehouse para exibir o painel Explorer, incluindo a pasta **orders**.
3. No menu superior, clique em **Abrir notebook**, **Notebook existente** e, em seguida, abra o notebook criado anteriormente. O notebook abrir√° ao lado do painel Explorer. Expanda Lakehouses, expanda a lista Arquivos e selecione a pasta "orders". Os arquivos CSV que voc√™ carregou s√£o listados ao lado do editor de notebook, desta maneira:

![Imagem da tela de arquivos csv na exibi√ß√£o do Explorer.](Images/explorer-notebook-view.jpg)

4. No menu ... de 2019.csv, selecione **Carregar dados** > **Spark**. O c√≥digo √© gerado automaticamente em uma nova c√©lula de c√≥digo:

```python
df = spark.read.format("csv").option("header","true").load("Files/orders/2019.csv")
# df now is a Spark DataFrame containing CSV data from "Files/orders/2019.csv".
display(df)
```

**Dica:** voc√™ pode ocultar os pain√©is Explorer do lakehouse √† esquerda usando os √≠cones ¬´. Isso d√° mais espa√ßo para o notebook.

5. Clique em ‚ñ∑ **Executar c√©lula** √† esquerda da c√©lula para executar o c√≥digo.

**OBSERVA√á√ÉO**: como esta √© a primeira vez que voc√™ executa o c√≥digo Spark, uma sess√£o do Spark ser√° iniciada. Isso pode levar alguns segundos ou mais. As execu√ß√µes subsequentes na mesma sess√£o ser√£o mais r√°pidas.

6. Quando o c√≥digo da c√©lula for conclu√≠do, analise a sa√≠da abaixo da c√©lula, que deve ser semelhante a:
 
![Imagem da tela mostrando c√≥digo e dados gerados automaticamente.](Images/auto-generated-load.jpg)

7. A sa√≠da mostra os dados do arquivo 2019.csv exibidos em linhas e as colunas.  Perceba que os cabe√ßalhos das colunas cont√™m a primeira linha dos dados. Para corrigir isso, voc√™ precisa modificar a primeira linha do c√≥digo da seguinte maneira:

```python
df = spark.read.format("csv").option("header","false").load("Files/orders/2019.csv")
```

8. Execute o c√≥digo novamente, para que o DataFrame identifique corretamente a primeira linha como dados. Perceba que os nomes das colunas agora mudaram para _c0, _c1, etc.

9. Nomes de coluna descritivos ajudam voc√™ a entender os dados. Para criar nomes de coluna significativos, voc√™ precisa definir o esquema e os tipos de dados. Voc√™ tamb√©m precisa importar um conjunto padr√£o de tipos SQL do Spark para definir os tipos de dados. Substitua o c√≥digo existente pelo seguinte:

```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
    ])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/2019.csv")

display(df)

```
10. Execute a c√©lula e analise a sa√≠da:

![Imagem da tela do c√≥digo com esquema definido e dados.](Images/define-schema.jpg)

11. Esse DataFrame s√≥ inclui os dados do arquivo 2019.csv. Modifique o c√≥digo para que o caminho do arquivo use um curinga * para ler todos os dados na pasta orders:

```python
from pyspark.sql.types import *

orderSchema = StructType([
    StructField("SalesOrderNumber", StringType()),
    StructField("SalesOrderLineNumber", IntegerType()),
    StructField("OrderDate", DateType()),
    StructField("CustomerName", StringType()),
    StructField("Email", StringType()),
    StructField("Item", StringType()),
    StructField("Quantity", IntegerType()),
    StructField("UnitPrice", FloatType()),
    StructField("Tax", FloatType())
    ])

df = spark.read.format("csv").schema(orderSchema).load("Files/orders/*.csv")

display(df)
```

12. Ao executar o c√≥digo modificado, voc√™ ver√° as vendas de 2019, 2020 e 2021. Apenas um subconjunto das linhas √© exibido, portanto, talvez voc√™ n√£o veja as linhas de todos os anos.

**OBSERVA√á√ÉO:** Voc√™ pode ocultar ou exibir a sa√≠da de uma c√©lula clicando em ... ao lado do resultado. Isso facilita o trabalho em um notebook.

## Explorar dados em um DataFrame

O objeto DataFrame fornece funcionalidades adicionais, como a capacidade de filtrar, agrupar e manipular dados.

### Filtrar um DataFrame

1. Adicione uma c√©lula de c√≥digo clicando em **+ C√≥digo**, que aparece quando voc√™ passa o mouse acima ou abaixo da c√©lula atual ou da respectiva sa√≠da. Como alternativa, no menu da faixa de op√ß√µes, selecione **Editar** e **+ Adicionar** c√©lula de c√≥digo.

2.  O c√≥digo a seguir filtrar√° os dados para que apenas duas colunas sejam retornadas. Ele tamb√©m usa *count* e *distinct* para resumir o n√∫mero de registros:

```python
customers = df['CustomerName', 'Email']

print(customers.count())
print(customers.distinct().count())

display(customers.distinct())
```

3. Execute o c√≥digo e analise a sa√≠da:

* O c√≥digo cria um novo DataFrame chamado **customers**, que cont√©m um subconjunto de colunas do DataFrame **df** original. Ao executar uma transforma√ß√£o DataFrame, voc√™ n√£o modifica o DataFrame original, mas retorna um novo.
* Outra maneira de obter o mesmo resultado √© usar o m√©todo select:

```
customers = df.select("CustomerName", "Email")
```

* As fun√ß√µes *count* e *distinct* do DataFrame s√£o usadas para fornecer totais do n√∫mero de clientes e de clientes exclusivos.

4. Modifique a primeira linha do c√≥digo usando *select* com uma fun√ß√£o *where* da seguinte maneira:

```python
customers = df.select("CustomerName", "Email").where(df['Item']=='Road-250 Red, 52')
print(customers.count())
print(customers.distinct().count())

display(customers.distinct())
```

5. Execute o c√≥digo modificado para selecionar apenas os clientes que compraram o produto Road-250 Red, 52. Voc√™ pode "encadear" v√°rias fun√ß√µes para que a sa√≠da de uma fun√ß√£o se torne a entrada da pr√≥xima. Nesse caso, o DataFrame criado pelo m√©todo *select* √© o DataFrame de origem do m√©todo **where** usado para aplicar crit√©rios de filtragem.

### Agregar e agrupar dados em um DataFrame

1. Adicione uma c√©lula de c√≥digo e insira o seguinte c√≥digo:

```python
productSales = df.select("Item", "Quantity").groupBy("Item").sum()

display(productSales)
```

2. Execute o c√≥digo. Observe que os resultados mostram a soma das quantidades de pedido agrupadas por produto. O m√©todo *groupBy* agrupa as linhas por Item, e a fun√ß√£o de agrega√ß√£o *sum* subsequente √© aplicada √†s colunas num√©ricas restantes, nesse caso, *Quantidade*.

3. Adicione outra c√©lula de c√≥digo ao notebook e insira o seguinte c√≥digo nela:

```python
from pyspark.sql.functions import *

yearlySales = df.select(year(col("OrderDate")).alias("Year")).groupBy("Year").count().orderBy("Year")

display(yearlySales)
```

4. Execute a c√©lula. Examine a sa√≠da. Os resultados mostram o n√∫mero de pedidos de vendas por ano:

* A instru√ß√£o *import* permite que voc√™ use a biblioteca SQL do Spark.
* O m√©todo *select* √© usado com uma fun√ß√£o year do SQL para extrair o componente de ano do campo *OrderDate*.
* O m√©todo *alias* para atribuir um nome de coluna ao valor de ano extra√≠do.
* O m√©todo *groupBy* agrupa os dados pela coluna Year derivada.
* A contagem de linhas em cada grupo √© calculada antes de o m√©todo *orderBy* ser usado para classificar o DataFrame resultante.

![Imagem de tela mostrando os resultados da agrega√ß√£o e agrupamento de dados em um DataFrame.](Images/spark-sql-dataframe.jpg)

## Usar o Spark para transformar arquivos de dados

Uma tarefa comum para engenheiros e cientistas de dados √© transformar os dados para processamento ou an√°lise downstream adicionais.

### Usar m√©todos e fun√ß√µes de DataFrame para transformar dados

1. Adicione uma c√©lula de c√≥digo ao notebook e insira o seguinte:

```python
from pyspark.sql.functions import *

# Create Year and Month columns
transformed_df = df.withColumn("Year", year(col("OrderDate"))).withColumn("Month", month(col("OrderDate")))

# Create the new FirstName and LastName fields
transformed_df = transformed_df.withColumn("FirstName", split(col("CustomerName"), " ").getItem(0)).withColumn("LastName", split(col("CustomerName"), " ").getItem(1))

# Filter and reorder columns
transformed_df = transformed_df["SalesOrderNumber", "SalesOrderLineNumber", "OrderDate", "Year", "Month", "FirstName", "LastName", "Email", "Item", "Quantity", "UnitPrice", "Tax"]

# Display the first five orders
display(transformed_df.limit(5))
```

2. Execute a c√©lula. Um novo DataFrame com base nos dados do pedido original com as seguintes transforma√ß√µes:

- Colunas Year e Month adicionadas com base na coluna OrderDate.
- Colunas FirstName e LastName adicionadas com base na coluna CustomerName.
- As colunas s√£o filtradas e reordenadas, e a coluna CustomerName √© removida.

3. Analise a sa√≠da e verifique se as transforma√ß√µes foram feitas nos dados.

Voc√™ pode usar a biblioteca do Spark SQL para transformar os dados filtrando linhas, derivando, removendo, renomeando colunas e aplicando outras modifica√ß√µes de dados.

>[!TIP]
> Confira a documenta√ß√£o do [dataframe do Apache Spark](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/dataframe.html) para saber mais sobre o objeto Dataframe.

### Salvar os dados transformados

A esta altura, talvez voc√™ queira salvar os dados transformados para us√°-los para an√°lise posterior.

O *Parquet* √© um formato popular de armazenamento de dados porque armazena dados com efici√™ncia e √© compat√≠vel com a maioria dos sistemas de an√°lise de dados em grande escala. Na verdade, √†s vezes, o requisito de transforma√ß√£o de dados √© converter dados de um formato, como CSV, em Parquet.

1. Para salvar o DataFrame transformado no formato Parquet, adicione uma c√©lula de c√≥digo e adicione o seguinte c√≥digo:  

```python
transformed_df.write.mode("overwrite").parquet('Files/transformed_data/orders')

print ("Transformed data saved!")
```

2. Execute a c√©lula e aguarde a mensagem indicando que os dados foram salvos. Em seguida, no painel do Lakehouses √† esquerda, no menu ... do n√≥ Arquivos, clique em **Atualizar**. Escolha a pasta transformed_data para verificar se ela cont√©m uma nova pasta chamada orders, que por sua vez cont√©m um ou mais arquivos Parquet.

3. Adicione uma c√©lula com o seguinte c√≥digo:

```python
orders_df = spark.read.format("parquet").load("Files/transformed_data/orders")
display(orders_df)
```

4. Execute a c√©lula.  Um novo DataFrame √© criado a partir dos arquivos parquet na pasta *transformed_data/orders*. Verifique se os resultados mostram os dados do pedido que foram carregados a partir dos arquivos Parquet.

![Imagem da tela mostrando arquivos Parquet.](Images/parquet-files.jpg)

### Salvar os dados em arquivos particionados

Ao lidar com grandes volumes de dados, o particionamento pode melhorar significativamente o desempenho e facilitar a filtragem de dados.

1. Adicione uma c√©lula com o c√≥digo para salvar o dataframe, particionando os dados por Year e Month:

```python
orders_df.write.partitionBy("Year","Month").mode("overwrite").parquet("Files/partitioned_data")

print ("Transformed data saved!")
```

2.  Execute a c√©lula e aguarde a mensagem indicando que os dados foram salvos. Em seguida, no painel do Lakehouses √† esquerda, no menu ... do n√≥ Arquivos, clique em **Atualizar** e expanda a pasta partitioned_orders para verificar que ela cont√©m uma hierarquia de pastas chamada *Year=xxxx*, cada uma contendo pastas chamadas *Month=xxxx*. Cada pasta mensal cont√©m um arquivo Parquet com os pedidos desse m√™s.

![Imagem da tela mostrando dados particionados por Year e Month.](Images/partitioned-data.jpg)

3. Adicione uma nova c√©lula com o seguinte c√≥digo para carregar um novo DataFrame a partir do arquivo orders.parquet:

```python
orders_2021_df = spark.read.format("parquet").load("Files/partitioned_data/Year=2021/Month=*")

display(orders_2021_df)
```

4. Execute a c√©lula e verifique se os resultados mostram os dados do pedido de vendas em 2021. Perceba que as colunas de particionamento especificadas no caminho (Year e Month) n√£o est√£o inclu√≠das no DataFrame.

## Trabalhar com tabelas e o SQL

Como voc√™ viu, os m√©todos nativos do objeto DataFrame permitem que voc√™ consulte e analise os dados a partir de um arquivo. No entanto, voc√™ pode se sentir mais confort√°vel trabalhando com tabelas usando a sintaxe SQL. O Spark fornece um metastore no qual voc√™ pode definir tabelas relacionais. 

A biblioteca do Spark SQL permite o uso de instru√ß√µes SQL para consultar tabelas no metastore. Isso d√° a flexibilidade de um data lake com o esquema de dados estruturado e as consultas baseadas em SQL de um data warehouse relacional, da√≠ o termo "data lakehouse".

### Criar uma tabela

As tabelas em um metastore do Spark s√£o abstra√ß√µes relacionais em arquivos no data lake. As tabelas podem ser *gerenciadas* pelo metastore ou *externas* e gerenciadas de forma independente do metastore.

1.  Adicione uma c√©lula de c√≥digo ao notebook e insira o seguinte c√≥digo, que salva o DataFrame dos dados do pedido de vendas como uma tabela chamada *salesorders*:

```python
# Create a new table
df.write.format("delta").saveAsTable("salesorders")

# Get the table description
spark.sql("DESCRIBE EXTENDED salesorders").show(truncate=False)
```

>[!NOTE]
> Neste exemplo, nenhum caminho expl√≠cito √© fornecido; portanto, os arquivos da tabela ser√£o gerenciados pelo metastore. Al√©m disso, a tabela √© salva no formato delta, que adiciona recursos de banco de dados relacional √†s tabelas. Isso inclui suporte para transa√ß√µes, controle de vers√£o de linha e outros recursos √∫teis. A cria√ß√£o de tabelas no formato delta √© preferencial para data lakehouses no Fabric.

2. Execute a c√©lula de c√≥digo e analise a sa√≠da, que descreve a defini√ß√£o da nova tabela.

3. No painel **Lakehouses**, no menu ... da pasta Tabelas, clique em **Atualizar**. Em seguida, expanda o n√≥ **Tabelas** e verifique se a tabela **salesorders** foi criada.

![Imagem da tela mostrando que a tabela salesorders foi criada.](Images/salesorders-table.jpg)

4. No menu ‚Ä¶ da tabela salesorders, clique em **Carregar dados** > **Spark**. Uma nova c√©lula de c√≥digo que cont√©m um c√≥digo semelhante ao seguinte exemplo √© adicionada:

```pyspark
df = spark.sql("SELECT * FROM [your_lakehouse].salesorders LIMIT 1000")

display(df)
```

5. Execute o novo c√≥digo, que usa a biblioteca do Spark SQL para inserir uma consulta SQL na tabela *salesorder* no c√≥digo PySpark e carregar os resultados da consulta em um DataFrame.

### Executar um c√≥digo SQL em uma c√©lula

Embora seja √∫til a inser√ß√£o de instru√ß√µes SQL em uma c√©lula que cont√©m c√≥digo PySpark, os analistas de dados muitas vezes preferem trabalhar diretamente com SQL.

1. Adicione uma nova c√©lula de c√≥digo ao notebook e insira o seguinte c√≥digo:

```SparkSQL
%%sql
SELECT YEAR(OrderDate) AS OrderYear,
       SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue
FROM salesorders
GROUP BY YEAR(OrderDate)
ORDER BY OrderYear;
```

7. Execute a c√©lula e analise os resultados. Observe que:

* O comando **%%sql** no in√≠cio da c√©lula (chamado de magic) altera a linguagem para Spark SQL em vez de PySpark.
* O c√≥digo SQL referencia a tabela *salesorders* que voc√™ j√° criou.
* A sa√≠da da consulta SQL √© exibida automaticamente como o resultado abaixo da c√©lula.

>[!NOTE]
> Para mais informa√ß√µes sobre o Spark SQL e os dataframes, confira a documenta√ß√£o do [Apache Spark SQL](https://spark.apache.org/sql/).

## Visualizar os dados com o Spark

Os gr√°ficos ajudam voc√™ a ver padr√µes e tend√™ncias com mais rapidez do que seria poss√≠vel verificando milhares de linhas de dados. Os notebooks do Fabric incluem uma exibi√ß√£o de gr√°fico integrada, mas n√£o foram projetados para gr√°ficos complexos. Para obter mais controle sobre como os gr√°ficos s√£o criados a partir de dados em DataFrames, use bibliotecas gr√°ficas em Python como *matplotlib* ou *seaborn*.

### Exibir os resultados como um gr√°fico

1. Adicione uma nova c√©lula de c√≥digo e insira o seguinte c√≥digo:

```python
%%sql
SELECT * FROM salesorders
```

2. Execute o c√≥digo para exibir dados da exibi√ß√£o salesorders que voc√™ criou anteriormente. Na se√ß√£o de resultados abaixo da c√©lula, altere a op√ß√£o **Exibir** de **Tabela** para **Gr√°fico**.

3.  Use o bot√£o **Personalizar gr√°fico** no canto superior direito do gr√°fico para definir as seguintes op√ß√µes:

* Tipo de gr√°fico: Gr√°fico de barras
* Chave: Item
* Valores: Quantidade
* Grupo de S√©ries: deixe em branco
* Agrega√ß√£o: Soma
* Empilhado: N√£o selecionado

Ao terminar, clique em **Aplicar**.

4. Seu gr√°fico ser√° semelhante a este:

![Imagem da tela da visualiza√ß√£o do gr√°fico do notebook do Fabric.](Images/built-in-chart.jpg) 

### Introdu√ß√£o √† matplotlib

1. Adicione uma nova c√©lula de c√≥digo e insira o seguinte c√≥digo:

```python
sqlQuery = "SELECT CAST(YEAR(OrderDate) AS CHAR(4)) AS OrderYear, \
                SUM((UnitPrice * Quantity) + Tax) AS GrossRevenue \
            FROM salesorders \
            GROUP BY CAST(YEAR(OrderDate) AS CHAR(4)) \
            ORDER BY OrderYear"
df_spark = spark.sql(sqlQuery)
df_spark.show()
```

2. Execute o c√≥digo. Retornar√° um DataFrame do Spark contendo a receita anual. Para visualizar os dados como um gr√°fico, come√ßaremos usando a biblioteca matplotlib em Python. Essa biblioteca √© a biblioteca de plotagem principal na qual muitas outras se baseiam e fornece muita flexibilidade na cria√ß√£o de gr√°ficos.

3. Adicione uma nova c√©lula de c√≥digo e o seguinte c√≥digo:

```python
from matplotlib import pyplot as plt

# matplotlib requires a Pandas dataframe, not a Spark one
df_sales = df_spark.toPandas()

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'])

# Display the plot
plt.show()
```

4. Execute a c√©lula e analise os resultados, que consistem em um gr√°fico de colunas com a receita bruta total de cada ano. Examine o c√≥digo e observe o seguinte:

* A biblioteca matplotlib requer um DataFrame do Pandas; portanto, voc√™ precisa converter o DataFrame do Spark retornado pela consulta Spark SQL.
* No n√∫cleo da biblioteca matplotlib est√° o objeto *pyplot*. Essa √© a base para a maioria das funcionalidades de plotagem.
* As configura√ß√µes padr√£o resultam em um gr√°fico utiliz√°vel, mas h√° muitas possibilidades de personaliza√ß√£o.

5.  Modifique o c√≥digo para plotar o gr√°fico da seguinte maneira:

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')

# Customize the chart
plt.title('Revenue by Year')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=45)

# Show the figure
plt.show()
```

6. Execute novamente a c√©lula de c√≥digo e veja os resultados. Agora est√° mais f√°cil de entender o gr√°fico.
7. Um gr√°fico est√° contido com uma Figura. Nos exemplos anteriores, a figura foi criada implicitamente, mas pode ser criada explicitamente. Modifique o c√≥digo para plotar o gr√°fico da seguinte maneira:

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a Figure
fig = plt.figure(figsize=(8,3))

# Create a bar plot of revenue by year
plt.bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')

# Customize the chart
plt.title('Revenue by Year')
plt.xlabel('Year')
plt.ylabel('Revenue')
plt.grid(color='#95a5a6', linestyle='--', linewidth=2, axis='y', alpha=0.7)
plt.xticks(rotation=45)

# Show the figure
plt.show()
```

8. Execute novamente a c√©lula de c√≥digo e veja os resultados. A figura determina a forma e o tamanho do gr√°fico.
9. Uma figura pode conter v√°rios subgr√°ficos, cada um em um eixo pr√≥prio. Modifique o c√≥digo para plotar o gr√°fico da seguinte maneira:

```python
from matplotlib import pyplot as plt

# Clear the plot area
plt.clf()

# Create a figure for 2 subplots (1 row, 2 columns)
fig, ax = plt.subplots(1, 2, figsize = (10,4))

# Create a bar plot of revenue by year on the first axis
ax[0].bar(x=df_sales['OrderYear'], height=df_sales['GrossRevenue'], color='orange')
ax[0].set_title('Revenue by Year')

# Create a pie chart of yearly order counts on the second axis
yearly_counts = df_sales['OrderYear'].value_counts()
ax[1].pie(yearly_counts)
ax[1].set_title('Orders per Year')
ax[1].legend(yearly_counts.keys().tolist())

# Add a title to the Figure
fig.suptitle('Sales Data')

# Show the figure
plt.show()
```

10. Execute novamente a c√©lula de c√≥digo e veja os resultados. 

>[!NOTE] 
> Para saber mais sobre plotagem com a matplotlib, confira a documenta√ß√£o da [matplotlib](https://matplotlib.org/).

### Usar a biblioteca seaborn

Embora a *matplotlib* permita que voc√™ crie diferentes tipos de gr√°fico, a biblioteca pode precisar de c√≥digo complexo para obter os melhores resultados. Por esse motivo, novas bibliotecas foram criadas com base na matplotlib para abstrair a complexidade e aprimorar as funcionalidades. Uma dessas bibliotecas √© a seaborn.

1. Adicione uma nova c√©lula de c√≥digo ao notebook e insira o seguinte c√≥digo: 

```python
import seaborn as sns

# Clear the plot area
plt.clf()

# Create a bar chart
ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)

plt.show()
```

2. Execute o c√≥digo para exibir um gr√°fico de barras usando a biblioteca seaborn.
3. Modifique o c√≥digo da seguinte maneira:

```python
import seaborn as sns

# Clear the plot area
plt.clf()

# Set the visual theme for seaborn
sns.set_theme(style="whitegrid")

# Create a bar chart
ax = sns.barplot(x="OrderYear", y="GrossRevenue", data=df_sales)

plt.show()
```

4.  Execute o c√≥digo modificado e observe que a seaborn permite que voc√™ defina um tema de cor para os gr√°ficos.
5.  Modifique o c√≥digo novamente da seguinte maneira:

```python
import seaborn as sns

# Clear the plot area
plt.clf()

# Create a line chart
ax = sns.lineplot(x="OrderYear", y="GrossRevenue", data=df_sales)

plt.show()
```

6.  Execute o c√≥digo modificado para ver a receita anual como um gr√°fico de linhas.

>[!NOTE]
> Para saber mais sobre plotagem com a seaborn, confira a documenta√ß√£o da [seaborn](https://seaborn.pydata.org/index.html).

## Limpar os recursos

Neste exerc√≠cio, voc√™ aprendeu a usar o Spark para trabalhar com dados no Microsoft Fabric.

Se voc√™ tiver terminado de explorar seus dados, encerre a sess√£o do Spark e exclua o espa√ßo de trabalho criado para este exerc√≠cio.

1.  No menu do notebook, selecione **Parar sess√£o** para encerrar a sess√£o do Spark.
1.  Na barra √† esquerda, selecione o √≠cone do workspace para ver todos os itens que ele cont√©m.
1.  Clique em **Configura√ß√µes do espa√ßo de trabalho** e, na se√ß√£o **Geral**, role para baixo e selecione **Remover este espa√ßo de trabalho**.
1.  Clique em **Excluir** para excluir o espa√ßo de trabalho.

