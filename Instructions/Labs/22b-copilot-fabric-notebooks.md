---
lab:
  title: Analisar dados com o Apache Spark e o Copilot em notebooks do Microsoft Fabric
  module: Get started with Copilot in Fabric for data engineering
---

# Analisar dados com o Apache Spark e o Copilot em notebooks do Microsoft Fabric

Neste laboratÃ³rio, usamos o Copilot para Engenharia de Dados do Fabric para carregar, transformar e salvar dados em um Lakehouse usando um notebook. Os notebooks fornecem um ambiente interativo que combina cÃ³digo, visualizaÃ§Ãµes e texto narrativo em um sÃ³ documento. Esse formato facilita documentar seu fluxo de trabalho, explicar seu raciocÃ­nio e compartilhar resultados com outras pessoas. Usando notebooks, vocÃª pode desenvolver e testar cÃ³digo iterativamente, visualizar dados em cada etapa e manter um registro claro do processo de anÃ¡lise. Essa abordagem aprimora a colaboraÃ§Ã£o, a reprodutibilidade e a compreensÃ£o, tornando os notebooks uma ferramenta ideal para tarefas de anÃ¡lise e engenharia de dados.

Tradicionalmente, trabalhar com notebooks para engenharia de dados exige que vocÃª escreva cÃ³digo em linguagens como Python ou Scala e tenha uma compreensÃ£o sÃ³lida de estruturas e bibliotecas como Apache Spark e pandas. Isso pode ser um desafio para desenvolvedores de programaÃ§Ã£o iniciantes ou que nÃ£o estÃ£o familiarizados com essas ferramentas. Com o Copilot nos notebooks do Fabric, vocÃª pode descrever suas tarefas de dados em linguagem natural e o Copilot gerarÃ¡ o cÃ³digo necessÃ¡rio para vocÃª, lidando com grande parte da complexidade tÃ©cnica e permitindo que vocÃª foque a anÃ¡lise.

Este exercÃ­cio deve levar aproximadamente **30** minutos para ser concluÃ­do.

## O que vocÃª aprenderÃ¡

Depois de concluir este laboratÃ³rio, vocÃª serÃ¡ capaz de:

- Criar e configurar um workspace e um lakehouse do Microsoft Fabric para tarefas de engenharia de dados.
- Usar o Copilot em notebooks do Fabric para gerar cÃ³digo de prompts em linguagem natural.
- Ingerir, limpar e transformar dados usando fluxos de trabalho assistidos pelo Copilot e o Apache Spark.
- Normalizar e preparar conjuntos de dados estatÃ­sticos para anÃ¡lise dividindo, filtrando e convertendo tipos de dados.
- Salvar os dados transformados como uma tabela no lakehouse para anÃ¡lise downstream.
- Usar o Copilot para gerar consultas e visualizaÃ§Ãµes para validaÃ§Ã£o e exploraÃ§Ã£o de dados.
- Entender as prÃ¡ticas recomendadas para limpeza de dados, transformaÃ§Ã£o e anÃ¡lise colaborativa no Microsoft Fabric.

## Antes de comeÃ§ar

VocÃª precisa de uma [Capacidade do Microsoft Fabric (F2 ou superior)](https://learn.microsoft.com/fabric/fundamentals/copilot-enable-fabric) com Copilot habilitado para concluir este exercÃ­cio.

> **ObservaÃ§Ã£o**: para sua conveniÃªncia, um notebook com todos os prompts deste exercÃ­cio estÃ¡ disponÃ­vel para download em:

`https://github.com/MicrosoftLearning/mslearn-fabric/raw/refs/heads/main/Allfiles/Labs/22b/Starter/eurostat-notebook.ipynb`

## CenÃ¡rio do exercÃ­cio

Vamos imaginar que a Contoso Health, uma rede de hospital de vÃ¡rias especialidades, quer expandir seus serviÃ§os na UE e deseja analisar os dados de populaÃ§Ã£o projetos. Este exemplo usa o conjunto dados de projeÃ§Ã£o de populaÃ§Ã£o [Eurostat](https://ec.europa.eu/eurostat/web/main/home) (escritÃ³rio estatÃ­stico da UniÃ£o Europeia).

Fonte: EUROPOP2023 PopulaÃ§Ã£o em 1Âº de janeiro por idade, sexo e tipo de projeÃ§Ã£o [[proj_23np](https://ec.europa.eu/eurostat/databrowser/product/view/proj_23np?category=proj.proj_23n)], Ãºltima atualizaÃ§Ã£o em 28 de junho de 2023.

## Criar um workspace

Antes de trabalhar com os dados no Fabric, crie um workspace com o Fabric habilitado. Um workspace no Microsoft Fabric serve como um ambiente colaborativo em que vocÃª pode organizar e gerenciar todos os seus artefatos de engenharia de dados, incluindo lakehouses, notebooks e conjuntos de dados. Pense nele como uma pasta de projeto que contÃ©m todos os recursos necessÃ¡rios para sua anÃ¡lise de dados.

1. Navegue atÃ© a [home page do Microsoft Fabric](https://app.fabric.microsoft.com/home?experience=fabric) em `https://app.fabric.microsoft.com/home?experience=fabric` em um navegador e entre com suas credenciais do Fabric.

1. Na barra de menus Ã  esquerda, selecione **Workspaces** (o Ã­cone Ã© semelhante a &#128455;).

1. Crie um workspace com um nome de sua escolha selecionando um modo de licenciamento que inclua a capacidade do Fabric (*Premium* ou *Fabric*). Observe que nÃ£o hÃ¡ suporte para *AvaliaÃ§Ã£o*.
   
    > **Por que isso importa**: o Copilot requer uma capacidade paga do Fabric para funcionar. Isso garante que vocÃª tenha acesso aos recursos baseados em IA que ajudarÃ£o a gerar cÃ³digo em todo o laboratÃ³rio.

1. Quando o novo workspace for aberto, ele estarÃ¡ vazio.

    ![Captura de tela de um espaÃ§o de trabalho vazio no Fabric.](./Images/new-workspace.png)

## Criar um lakehouse

Agora que vocÃª tem um espaÃ§o de trabalho, Ã© hora de criar um lakehouse no qual vocÃª ingerirÃ¡ os dados. Um lakehouse combina os benefÃ­cios de um data lake (armazenamento de dados brutos em vÃ¡rios formatos) com um data warehouse (dados estruturados otimizados para anÃ¡lise). Ele servirÃ¡ como o local de armazenamento para nossos dados brutos de populaÃ§Ã£o e o destino para nosso conjunto de dados limpo e transformado.

1. Na barra de menus Ã  esquerda, selecione **Criar**. Na pÃ¡gina *Novo*, na seÃ§Ã£o *Engenharia de Dados*, selecione **Lakehouse**. DÃª um nome exclusivo de sua preferÃªncia.

    >**ObservaÃ§Ã£o**: se a opÃ§Ã£o **Criar** nÃ£o estiver fixada na barra lateral, vocÃª precisarÃ¡ selecionar a opÃ§Ã£o de reticÃªncias (**...**) primeiro.

![Captura de tela do botÃ£o Criar no Fabric.](./Images/copilot-fabric-notebook-create.png)

ApÃ³s alguns minutos, um lakehouse vazio serÃ¡ criado.

![Captura de tela de um novo lakehouse.](./Images/new-lakehouse.png)

## Criar um notebook

Agora vocÃª pode criar um notebook do Fabric para trabalhar com seus dados. Os notebooks fornecem um ambiente interativo em que vocÃª pode escrever e executar cÃ³digo, visualizar resultados e documentar seu processo de anÃ¡lise de dados. Eles sÃ£o ideais para anÃ¡lise exploratÃ³ria de dados e desenvolvimento iterativo, permitindo que vocÃª veja os resultados de cada etapa imediatamente.

1. Na barra de menus Ã  esquerda, selecione **Criar**. Na pÃ¡gina *Novo*, na seÃ§Ã£o *Engenharia de Dados*, selecione **Notebook**.

    Um novo notebook chamado **Notebook 1** Ã© criado e aberto.

    ![Captura de tela de um novo notebook.](./Images/new-notebook.png)

1. O Fabric atribui um nome a cada notebook criado, como Bloco de Notebook 1, Notebook 2, etc. Clique no painel de nome acima da guia **PÃ¡gina Inicial** no menu para alterar o nome para algo mais descritivo.

    ![Captura de tela de um novo notebook, com a capacidade de renomear.](./Images/copilot-fabric-notebook-rename.png)

1. Selecione a primeira cÃ©lula (que atualmente Ã© uma cÃ©lula de cÃ³digo) e, na barra de ferramentas no canto superior direito, use o botÃ£o **Mâ†“** para convertÃª-la em uma cÃ©lula Markdown. O texto contido na cÃ©lula serÃ¡ entÃ£o exibido como texto formatado.

    > **Por que usar cÃ©lulas de markdown**: as cÃ©lulas markdown permitem documentar sua anÃ¡lise com texto formatado, tornando o notebook mais legÃ­vel e mais fÃ¡cil de entender para outras pessoas (ou para vocÃª mesmo quando vocÃª retornar a ele mais tarde).

    ![Captura de tela de um notebook, alterando a primeira cÃ©lula para se tornar markdown.](./Images/copilot-fabric-notebook-markdown.png)

1. Use o botÃ£o ğŸ–‰ (Editar) para alternar a cÃ©lula para o modo de ediÃ§Ã£o e modifique o Markdown como mostrado abaixo.

    ```md
    # Explore Eurostat population data.
    Use this notebook to explore population data from Eurostat
    ```
    
    ![Imagem de tela de um notebook Fabric com uma cÃ©lula Markdown.](Images/copilot-fabric-notebook-step-1-created.png)
    
    Quando terminar, clique em qualquer lugar no notebook fora da cÃ©lula para parar de editÃ¡-lo.

## Anexar a lakehouse ao notebook

Para trabalhar com dados no lakehouse do notebook, vocÃª precisa anexar o lakehouse ao notebook. Essa conexÃ£o permite que seu notebook leia e grave no armazenamento lakehouse, criando uma integraÃ§Ã£o perfeita entre o ambiente de anÃ¡lise e o armazenamento de dados.

1. Selecione seu novo espaÃ§o de trabalho na barra Ã  esquerda. VocÃª verÃ¡ uma lista de itens contidos no espaÃ§o de trabalho, incluindo seu lakehouse e notebook.

1. Selecione o lakehouse para exibir o painel do Explorador.

1. No menu superior, clique em **Abrir notebook**, **Notebook existente** e, em seguida, abra o notebook criado anteriormente. O notebook abrirÃ¡ ao lado do painel Explorer. Expanda Lakehouses e expanda a lista Arquivos. Observe que nÃ£o hÃ¡ tabelas ou arquivos listados ainda ao lado do editor do notebook, portanto:

    ![Imagem da tela de arquivos csv na exibiÃ§Ã£o do Explorer.](Images/copilot-fabric-notebook-step-2-lakehouse-attached.png)

    > **O que vocÃª vÃª**: o painel do Explorador Ã  esquerda mostra sua estrutura lakehouse. No momento, ele estÃ¡ vazio, mas Ã  medida que carregamos e processemos dados, vocÃª verÃ¡ arquivos aparecendo na seÃ§Ã£o **Arquivos** e tabelas aparecendo na seÃ§Ã£o **Tabelas**.


## Carregar dados

Agora, usaremos o Copilot para nos ajudar a baixar dados da API do Eurostat. Em vez de escrever cÃ³digo Python do zero, descreveremos o que queremos fazer em linguagem natural e o Copilot gerarÃ¡ o cÃ³digo apropriado. Isso demonstra um dos principais benefÃ­cios da codificaÃ§Ã£o assistida por IA: vocÃª pode se concentrar na lÃ³gica de negÃ³cios, em vez de nos detalhes da implementaÃ§Ã£o tÃ©cnica.

1. Crie uma cÃ©lula no notebook e copie a instruÃ§Ã£o a seguir para ela. Para indicar que queremos que o Copilot gere cÃ³digo, use `%%code` como a primeira instruÃ§Ã£o na cÃ©lula. 

    > **Sobre o comando mÃ¡gico `%%code`**: essa instruÃ§Ã£o especial informa ao Copilot que vocÃª deseja que ele gere cÃ³digo do Python com base em sua descriÃ§Ã£o de linguagem natural. Ã‰ um dos vÃ¡rios "comandos mÃ¡gicos" que ajudam vocÃª a interagir com o Copilot com mais eficiÃªncia.

    ```copilot-prompt
    %%code
    
    Download the following file from this URL:
    
    https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/proj_23np$defaultview/?format=TSV
     
    Then write the file to the default lakehouse into a folder named temp. Create the folder if it doesn't exist yet.
    ```
    
1. Clique em â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo.

    O Copilot gerou o cÃ³digo a seguir, que pode ser um pouco diferente dependendo do seu ambiente e das atualizaÃ§Ãµes mais recentes do Copilot.
    
    ![Captura de tela do cÃ³digo gerado pelo Copilot.](Images/copilot-fabric-notebook-step-3-code-magic.png)
    
    > **Como o Copilot funciona**: observe como o Copilot converte sua solicitaÃ§Ã£o de linguagem natural em cÃ³digo Python em funcionamento. Ele entende que vocÃª precisa fazer uma solicitaÃ§Ã£o HTTP, manipular o sistema de arquivos e salvar os dados em um local especÃ­fico em seu lakehouse.
    
    Aqui estÃ¡ o cÃ³digo completo para sua conveniÃªncia, caso vocÃª encontre exceÃ§Ãµes durante a execuÃ§Ã£o:
    
    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    import requests
    import os
    
    # Define the URL and the local path
    url = "https://ec.europa.eu/eurostat/api/dissemination/sdmx/2.1/data/proj_23np$defaultview/?format=TSV"
    local_path = "/lakehouse/default/Files/temp/"
    file_name = "proj_23np.tsv"
    file_path = os.path.join(local_path, file_name)
    
    # Create the temporary directory if it doesn't exist
    if not os.path.exists(local_path):
        os.makedirs(local_path)
    
    # Download the file
    response = requests.get(url)
    response.raise_for_status()  # Check that the request was successful
    
    # Write the content to the file
    with open(file_path, "wb") as file:
        file.write(response.content)
    
    print(f"File downloaded and saved to {file_path}")
    ```

1. Selecione â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo e observe a saÃ­da. O arquivo deve ser baixado e salvo na pasta temporÃ¡ria do Lakehouse.

    > **ObservaÃ§Ã£o**: talvez seja necessÃ¡rio atualizar os Arquivos do lakehouse selecionando os trÃªs pontos ...
    
    ![Captura de tela de um arquivo temporÃ¡rio criado no lakehouse.](Images/copilot-fabric-notebook-step-4-lakehouse-refreshed.png)

1. Agora que temos o arquivo de dados brutos no lakehouse, precisamos carregÃ¡-lo em um DataFrame do Spark para analisÃ¡-lo e transformÃ¡-lo. Crie uma cÃ©lula no notebook e copie a instruÃ§Ã£o a seguir para ela.

    > **InformaÃ§Ãµes**: um DataFrame Ã© uma coleÃ§Ã£o distribuÃ­da de dados organizados em colunas nomeadas, semelhante a uma tabela em um banco de dados ou em uma planilha.

    ```copilot-prompt
    %%code
    
    Load the file 'Files/temp/proj_23np.tsv' into a spark dataframe.
    
    The fields are separated with a tab.
    
    Show the contents of the DataFrame using display method.
    ```

1. Selecione â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo e observe a saÃ­da. O dataframe deve conter os dados do arquivo TSV. Veja um exemplo da aparÃªncia do cÃ³digo gerado:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Load the file 'Files/temp/proj_23np.tsv' into a spark dataframe.
    # The fields have been separated with a tab.
    file_path = "Files/temp/proj_23np.tsv"
    
    spark_df = spark.read.format("csv").option("delimiter", "\t").option("header", "true").load(file_path)
    
    # Show the contents of the DataFrame using display method
    display(spark_df)
    ```

Aqui estÃ¡ um exemplo de como a saÃ­da pode ser:

| freq,projection,sex,age,unit,geo\TIME_PERIOD |      2022  |      2023  |   ...  |      2.100  |
| -------------------------------------------- | ---------- | ---------- | ------ | ---------- |
|                         A,BSL,F,TOTAL,PER,AT |   4553444  |   4619179  |   ...  |   4807661  |
|                         A,BSL,F,TOTAL,PER,BE |   5883978  |   5947528  |   ...  |   6331785  |
|                         A,BSL,F,TOTAL,PER,BG |   3527626  |   3605059  |   ...  |   2543673  |
|                                          ... |       ...  |       ...  |   ...  |   5081250  |
|                         A,BSL,F,TOTAL,PER,CY |    463622  |    476907  |   ...  |    504781  |

> **NoÃ§Ãµes bÃ¡sicas sobre a estrutura de dados**: observe que a primeira coluna contÃ©m diversos valores separados por vÃ­rgulas (frequÃªncia, tipo de projeÃ§Ã£o, sexo, idade, unidade e localizaÃ§Ã£o geogrÃ¡fica), enquanto as demais colunas representam anos com valores de populaÃ§Ã£o. Essa estrutura Ã© comum em conjuntos de dados estatÃ­sticos, mas precisa ser limpa para anÃ¡lise eficaz.

## Transformar dados: dividir campos

Agora vamos transformar os dados. O primeiro campo deve ser dividido em colunas separadas. AlÃ©m disso, tambÃ©m Ã© preciso trabalhar com os tipos de dados corretos e aplicar a filtragem. 

> **Por que precisamos dividir os campos**: a primeira coluna contÃ©m vÃ¡rias informaÃ§Ãµes concatenadas (frequÃªncia, tipo de projeÃ§Ã£o, sexo, faixa etÃ¡ria, unidade e cÃ³digo geogrÃ¡fico). Para uma anÃ¡lise adequada, cada informaÃ§Ã£o deve estar em uma coluna prÃ³pria. Esse processo Ã© chamado de "normalizaÃ§Ã£o" da estrutura de dados.

1. Crie uma cÃ©lula no notebook e copie a instruÃ§Ã£o a seguir para ela.


    ```copilot-prompt
    %%code
    
    From the currently loaded DataFrame, split the first field 'freq,projection,sex,age,unit,geo\TIME_PERIOD' using a comma into 6 separate fields.
    ```

1. Selecione â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo e observe a saÃ­da. Aqui estÃ¡ um exemplo de como a saÃ­da pode ser:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import split, col
    
    # Split the first field 'freq,projection,sex,age,unit,geo\TIME_PERIOD' into 6 separate fields
    spark_df = spark_df.withColumn("freq", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(0)) \
                       .withColumn("projection", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(1)) \
                       .withColumn("sex", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(2)) \
                       .withColumn("age", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(3)) \
                       .withColumn("unit", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(4)) \
                       .withColumn("geo", split(col("freq,projection,sex,age,unit,geo\\TIME_PERIOD"), ",").getItem(5))
    
    # Show the updated DataFrame
    display(spark_df)
    ```

1. Clique em â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo. Talvez seja necessÃ¡rio rolar a tabela para a direita para ver os novos campos adicionados a ela.

    ![Captura de tela da tabela resultante com campos adicionais.](Images/copilot-fabric-notebook-split-fields.png)

## Transformar dados: remover campos

Alguns campos na tabela nÃ£o agregam nenhum valor significativo, pois contÃªm apenas uma entrada distinta. Como prÃ¡tica recomendada, devemos removÃª-los do conjunto de dados.

> **PrincÃ­pio de limpeza de dados**: Colunas com apenas um valor exclusivo nÃ£o fornecem valor analÃ­tico e podem tornar seu conjunto de dados desnecessariamente complexo. RemovÃª-las simplifica a estrutura de dados e melhora o desempenho. Nesse caso, "freq" (frequÃªncia), "age" (todos os registros mostram TOTAL) e "unit" (todos os registros mostram PER para pessoas) sÃ£o constantes em todas as linhas.

1. Crie uma cÃ©lula no notebook e copie a instruÃ§Ã£o a seguir para ela.

    ```copilot-prompt
    %%code
    
    From the currently loaded DataFrame, remove the fields 'freq', 'age', 'unit'.
    ```

1. Selecione â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo e observe a saÃ­da. Aqui estÃ¡ um exemplo de como a saÃ­da pode ser:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Remove the fields 'freq', 'age', 'unit'
    spark_df = spark_df.drop("freq", "age", "unit")
    
    # Show the updated DataFrame
    display(spark_df)
    ```

1. Clique em â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo.

## Transformar dados: reposicionar campos

Organizar seus dados com as colunas de identificaÃ§Ã£o mais importantes primeiro facilita a leitura e a compreensÃ£o. Na anÃ¡lise de dados, Ã© uma prÃ¡tica comum posicionar colunas categÃ³ricas/dimensionais (como tipo de projeÃ§Ã£o, sexo e localizaÃ§Ã£o geogrÃ¡fica) antes das colunas numÃ©ricas/de medida (os valores de populaÃ§Ã£o por ano).

1. Crie uma cÃ©lula no notebook e copie a instruÃ§Ã£o a seguir para ela.

    ```copilot-prompt
    %%code
    
    From the currently loaded DataFrame, the fields 'projection', 'sex', 'geo' should be positioned first.
    ```

1. Selecione â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo e observe a saÃ­da. Aqui estÃ¡ um exemplo de como a saÃ­da pode ser:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Reorder the DataFrame with 'projection', 'sex', 'geo' fields first
    new_column_order = ['projection', 'sex', 'geo'] + [col for col in spark_df.columns if col not in {'projection', 'sex', 'geo'}]
    spark_df = spark_df.select(new_column_order)
    
    # Show the reordered DataFrame
    display(spark_df)
    ```

1. Clique em â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo.

## Transformar dados: substituir valores

No momento, o campo de projeÃ§Ã£o contÃ©m cÃ³digos criptogrÃ¡ficos que nÃ£o sÃ£o amigÃ¡veis. Para melhor legibilidade e anÃ¡lise, substituiremos esses cÃ³digos por nomes descritivos que explicam claramente o que cada cenÃ¡rio de projeÃ§Ã£o representa.

> **NoÃ§Ãµes bÃ¡sicas sobre cenÃ¡rios de projeÃ§Ã£o**: as organizaÃ§Ãµes estatÃ­sticas geralmente usam cenÃ¡rios diferentes para modelar futuras alteraÃ§Ãµes de populaÃ§Ã£o. A linha de base representa o cenÃ¡rio mais provÃ¡vel, enquanto os testes de sensibilidade mostram como a populaÃ§Ã£o pode mudar sob suposiÃ§Ãµes diferentes sobre taxas de fertilidade, taxas de mortalidade e padrÃµes de migraÃ§Ã£o.

1. Crie uma cÃ©lula no notebook e copie a instruÃ§Ã£o a seguir para ela.


    ```copilot-prompt
    %%code
    
    The 'projection' field contains codes that should be replaced with the following values:
        _'BSL' -> 'Baseline projections'.
        _'LFRT' -> 'Sensitivity test: lower fertility'.
        _'LMRT' -> 'Sensitivity test: lower mortality'.
        _'HMIGR' -> 'Sensitivity test: higher migration'.
        _'LMIGR' -> 'Sensitivity test: lower migration'.
        _'NMIGR' -> 'Sensitivity test: no migration'.
    ```

1. Selecione â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo e observe a saÃ­da. Aqui estÃ¡ um exemplo de como a saÃ­da pode ser:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import when
    
    # Replace projection codes
    spark_df = spark_df.withColumn("projection", 
                                   when(spark_df["projection"] == "BSL", "Baseline projections")
                                   .when(spark_df["projection"] == "LFRT", "Sensitivity test: lower fertility")
                                   .when(spark_df["projection"] == "LMRT", "Sensitivity test: lower mortality")
                                   .when(spark_df["projection"] == "HMIGR", "Sensitivity test: higher migration")
                                   .when(spark_df["projection"] == "LMIGR", "Sensitivity test: lower migration")
                                   .when(spark_df["projection"] == "NMIGR", "Sensitivity test: no migration")
                                   .otherwise(spark_df["projection"]))
    
    # Display the updated DataFrame
    display(spark_df)
    ```

1. Clique em â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo.

    ![Captura de tela da tabela resultante com os valores de campo de projeto substituÃ­dos.](Images/copilot-fabric-notebook-replace-values.png)
    
## Transformar dados: filtrar dados

A tabela de projeÃ§Ãµes de populaÃ§Ã£o contÃ©m 2 linhas para paÃ­ses que nÃ£o existem: EU27_2020 (*totais da UniÃ£o Europeia - 27 paÃ­ses*) e EA20 (*Ã¡rea do Euro - 20 paÃ­ses*). Precisamos remover essas duas linhas, pois queremos manter os dados apenas com a granulaÃ§Ã£o mais baixa.

> **PrincÃ­pio de granularidade de dados**: para anÃ¡lise detalhada, Ã© importante trabalhar com dados no nÃ­vel mais granular possÃ­vel. Valores agregados (como totais da UE) sempre podem ser calculados quando necessÃ¡rio, mas incluÃ­-los em seu conjunto de dados base pode causar contagem duplicada ou confusÃ£o na anÃ¡lise.

![Captura de tela da tabela com as Ã¡reas geogrÃ¡ficas EA20 e EU2_2020 realÃ§adas.](Images/copilot-fabric-notebook-europe.png)

1. Crie uma cÃ©lula no notebook e copie a instruÃ§Ã£o a seguir para ela.

    ```copilot-prompt
    %%code
    
    Filter the 'geo' field and remove values 'EA20' and 'EU27_2020' (these are not countries).
    ```

1. Selecione â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo e observe a saÃ­da. Aqui estÃ¡ um exemplo de como a saÃ­da pode ser:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Filter out 'geo' values 'EA20' and 'EU27_2020'
    spark_df = spark_df.filter((spark_df['geo'] != 'EA20') & (spark_df['geo'] != 'EU27_2020'))
    
    # Display the filtered DataFrame
    display(spark_df)
    ```

1. Clique em â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo.

    A tabela de projeto de populaÃ§Ã£o tambÃ©m contÃ©m um campo "sexo" com os seguintes valores distintos:
    
    - M: masculino
    - F: feminino
    - T: total (masculino + feminino)

    Novamente, precisamos remover os totais, portanto, manteremos os dados no nÃ­vel mais baixo de detalhes.

    > **Por que remover os totais**: semelhante Ã s agregaÃ§Ãµes geogrÃ¡ficas, queremos manter apenas as categorias de sexo individuais (masculino e feminino) e excluir os valores totais. Isso permite uma anÃ¡lise mais flexÃ­vel. VocÃª sempre pode somar valores masculinos e femininos para obter totais, mas nÃ£o pode dividir totais de volta em componentes.

1. Crie uma cÃ©lula no notebook e copie a instruÃ§Ã£o a seguir para ela.

    ```copilot-prompt
    %%code
    
    Filter the 'sex' field and remove 'T' (these are totals).
    ```

1. Selecione â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo e observe a saÃ­da. Aqui estÃ¡ um exemplo de como a saÃ­da pode ser:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    # Filter out 'sex' values 'T'
    spark_df = spark_df.filter(spark_df['sex'] != 'T')
    
    # Display the filtered DataFrame
    display(spark_df)
    ```

1. Clique em â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo.

## Transformar dados: cortar espaÃ§os

Alguns nomes de campo na tabela de projeÃ§Ã£o de populaÃ§Ã£o tÃªm um espaÃ§o no final. Precisamos aplicar uma operaÃ§Ã£o de corte aos nomes desses campos.

> **PreocupaÃ§Ã£o sobre qualidade de dados**: espaÃ§os extras em nomes de coluna podem causar problemas ao consultar dados ou criar visualizaÃ§Ãµes. Ã‰ um problema comum de qualidade de dados, especialmente quando os dados vÃªm de fontes externas ou sÃ£o exportados de outros sistemas. A filtragem de espaÃ§os garante a consistÃªncia e evita problemas difÃ­ceis de depurar mais tarde.

1. Crie uma cÃ©lula no notebook e copie a instruÃ§Ã£o a seguir para ela.

    ```copilot-prompt
    %%code
    
    Strip spaces from all field names in the dataframe.
    ```

1. Selecione â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo e observe a saÃ­da. Aqui estÃ¡ um exemplo de como a saÃ­da pode ser:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import col
    
    # Strip spaces from all field names
    spark_df = spark_df.select([col(column).alias(column.strip()) for column in spark_df.columns])
    
    # Display the updated DataFrame
    display(spark_df)
    ```

1. Clique em â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo.

## Transformar dados: conversÃ£o de tipo de dados

Se quisermos analisar corretamente os dados mais tarde (usando o Power BI ou SQL, por exemplo), precisaremos verificar se os tipos de dados (como nÃºmeros e datetime) estÃ£o definidos corretamente. 

> **ImportÃ¢ncia dos tipos de dados corretos**: quando os dados sÃ£o carregados de arquivos de texto, todas as colunas sÃ£o inicialmente tratadas como cadeias de caracteres. A conversÃ£o de colunas de ano em inteiros permite operaÃ§Ãµes matemÃ¡ticas (como cÃ¡lculos e agregaÃ§Ãµes) e garante a classificaÃ§Ã£o adequada. Essa etapa Ã© crucial para ferramentas de anÃ¡lise e visualizaÃ§Ã£o downstream.

1. Crie uma cÃ©lula no notebook e copie a instruÃ§Ã£o a seguir para ela.

    ```copilot-prompt
    %%code
    
    Convert the data type of all the year fields to integer.
    ```

1. Selecione â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo e observe a saÃ­da. Aqui estÃ¡ um exemplo de como a saÃ­da pode ser:

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    from pyspark.sql.functions import col
    
    # Convert the data type of all the year fields to integer
    year_columns = [col(column).cast("int") for column in spark_df.columns if column.strip().isdigit()]
    spark_df = spark_df.select(*spark_df.columns[:3], *year_columns)
    
    # Display the updated DataFrame
    display(spark_df)
    ```
    
1. Clique em â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo. Aqui estÃ¡ um exemplo da aparÃªncia da saÃ­da (colunas e linhas removidas para fins de brevidade):

|          projeÃ§Ã£o|sexo|geo|    2022|    2023|     ...|    2.100|
|--------------------|---|---|--------|--------|--------|--------| 
|ProjeÃ§Ãµes de linha de base|  F| AT| 4553444| 4619179|     ...| 4807661|
|ProjeÃ§Ãµes de linha de base|  F| BE| 5883978| 5947528|     ...| 6331785|
|ProjeÃ§Ãµes de linha de base|  F| BG| 3527626| 3605059|     ...| 2543673|
|...                 |...|...|     ...|     ...|     ...|     ...|
|ProjeÃ§Ãµes de linha de base|  F| LU|  320333|  329401|     ...|  498954|

>[!TIP]
> Talvez seja necessÃ¡rio rolar a tabela para a direita para observar todas as colunas.

## Salvar dados

Em seguida, queremos salvar os dados transformados em nosso lakehouse. 

> **Por que salvar os dados transformados**: apÃ³s todo esse trabalho de limpeza e transformaÃ§Ã£o de dados, queremos manter os resultados. Salvar os dados como uma tabela no lakehouse permite que nÃ³s e outras pessoas usem esse conjunto de dados limpo para vÃ¡rios cenÃ¡rios de anÃ¡lise sem necessidade de repetir o processo de transformaÃ§Ã£o. Ele tambÃ©m permite que outras ferramentas no ecossistema do Microsoft Fabric (como Power BI, Ponto de Extremidade da AnÃ¡lise de SQL e Data Factory) funcionem com esses dados.

1. Crie uma cÃ©lula no notebook e copie a instruÃ§Ã£o a seguir para ela.

    ```copilot-prompt
    %%code
    
    Save the dataframe as a new table named 'Population' in the default lakehouse.
    ```
    
1. Clique em â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo. O Copilot gerou o cÃ³digo, que pode ser um pouco diferente dependendo do seu ambiente e das atualizaÃ§Ãµes mais recentes do Copilot.

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    spark_df.write.format("delta").saveAsTable("Population")
    ```

1. Clique em â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo.

## ValidaÃ§Ã£o: fazer perguntas

Agora vamos explorar o poder do Copilot para anÃ¡lise de dados. Em vez de escrever consultas SQL complexas ou cÃ³digo de visualizaÃ§Ã£o do zero, podemos fazer perguntas em linguagem natural do Copilot sobre nossos dados e ele gerarÃ¡ o cÃ³digo apropriado para respondÃª-las.

1. Para validar se os dados foram salvos corretamente, expanda as tabelas no Lakehouse e verifique o conteÃºdo (talvez seja necessÃ¡rio atualizar a pasta Tabelas selecionando os trÃªs pontos ...). 

    ![Captura de tela do lakehouse agora contendo uma nova tabela chamada "PopulaÃ§Ã£o".](Images/copilot-fabric-notebook-step-5-lakehouse-refreshed.png)

1. Na faixa de opÃ§Ãµes da PÃ¡gina Inicial, selecione a opÃ§Ã£o Copilot.

    > **Interface do Copilot Chat**: o painel do Copilot fornece uma interface de conversa em que vocÃª pode fazer perguntas sobre seus dados em linguagem natural. Ela pode gerar cÃ³digo para anÃ¡lise, criar visualizaÃ§Ãµes e ajudÃ¡-lo a explorar padrÃµes em seu conjunto de dados.

    ![Captura de tela do notebook com o painel do Copilot aberto.](Images/copilot-fabric-notebook-step-6-copilot-pane.png)

1. Digite a seguinte solicitaÃ§Ã£o:

    ```copilot-prompt
    What are the projected population trends for geo BE  from 2020 to 2050 as a line chart visualization. Make sure to sum up male and female numbers. Use only existing columns from the population table. Perform the query using SQL.
    ```

    > **O que isso demonstra**: este prompt demonstra a capacidade do Copilot de entender o contexto (nossa tabela PopulaÃ§Ã£o), gerar consultas SQL e criar visualizaÃ§Ãµes. Ele Ã© particularmente eficiente porque combina a consulta de dados com a visualizaÃ§Ã£o em uma sÃ³ solicitaÃ§Ã£o.

1. Examine a saÃ­da gerada, que pode ser um pouco diferente dependendo do seu ambiente e das atualizaÃ§Ãµes mais recentes do Copilot. Copie o fragmento de cÃ³digo em uma nova cÃ©lula.

    ```python
    #### ATTENTION: AI-generated code can include errors or operations you didn't intend. Review the code in this cell carefully before running it.
    
    import plotly.graph_objs as go
    
    # Perform the SQL query to get projected population trends for geo BE, summing up male and female numbers
    result = spark.sql(
        """
        SELECT projection, sex, geo, SUM(`2022`) as `2022`, SUM(`2023`) as `2023`, SUM(`2025`) as `2025`,
               SUM(`2030`) as `2030`, SUM(`2035`) as `2035`, SUM(`2040`) as `2040`,
               SUM(`2045`) as `2045`, SUM(`2050`) as `2050`
        FROM Population
        WHERE geo = 'BE' AND projection = 'Baseline projections'
        GROUP BY projection, sex, geo
        """
    )
    df = result.groupBy("projection").sum()
    df = df.orderBy("projection").toPandas()
    
    # Extract data for the line chart
    years = df.columns[1:].tolist()
    values = df.iloc[0, 1:].tolist()
    
    # Create the plot
    fig = go.Figure()
    fig.add_trace(go.Scatter(x=years, y=values, mode='lines+markers', name='Projected Population'))
    
    # Update the layout
    fig.update_layout(
        title='Projected Population Trends for Geo BE (Belgium) from 2022 to 2050',
        xaxis_title='Year',
        yaxis_title='Population',
        template='plotly_dark'
    )
    
    # Display the plot
    fig.show()
    ```

1. Clique em â–· **Executar cÃ©lula** Ã  esquerda da cÃ©lula para executar o cÃ³digo. 

    Observe o grÃ¡fico que ele criou:
    
    ![Captura de tela do notebook com o grÃ¡fico de linhas criado.](Images/copilot-fabric-notebook-step-7-line-chart.png)
    
    > **O que vocÃª realizou**: vocÃª usou o Copilot para gerar uma visualizaÃ§Ã£o que mostra tendÃªncias de populaÃ§Ã£o para a BÃ©lgica ao longo do tempo. Isso demonstra o fluxo de trabalho de engenharia de dados de ponta a ponta: ingestÃ£o, transformaÃ§Ã£o, armazenamento e anÃ¡lise de dados, tudo com assistÃªncia de IA.

## Limpar os recursos

Neste exercÃ­cio, vocÃª aprendeu a usar o Copilot e o Spark para trabalhar com os dados no Microsoft Fabric.

Se vocÃª tiver terminado de explorar seus dados, encerre a sessÃ£o do Spark e exclua o espaÃ§o de trabalho criado para este exercÃ­cio.

1.  No menu do notebook, selecione **Parar sessÃ£o** para encerrar a sessÃ£o do Spark.
1.  Na barra Ã  esquerda, selecione o Ã­cone do workspace para ver todos os itens que ele contÃ©m.
1.  Clique em **ConfiguraÃ§Ãµes do espaÃ§o de trabalho** e, na seÃ§Ã£o **Geral**, role para baixo e selecione **Remover este espaÃ§o de trabalho**.
1.  Clique em **Excluir** para excluir o espaÃ§o de trabalho.
